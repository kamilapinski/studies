{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1gX_p_w57g9"
      },
      "source": [
        "# Drugie duże zadanie zaliczeniowe na laboratorium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gutTuvLqlcrC",
        "outputId": "eaac6c43-11a4-4363-e0a4-6cc6f3a049f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "E1T4v9V6cQFM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, r2_score, f1_score, classification_report\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import optuna\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import plotly.express as px\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn .linear_model import ElasticNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "zrX1EPwlbmr6",
        "outputId": "9782e2e3-f532-461e-b5d4-8acc32cbfaed"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3ffeea00-14d7-4863-a0ee-6b47464979d6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Output</th>\n",
              "      <th>Input1</th>\n",
              "      <th>Input2</th>\n",
              "      <th>Input3</th>\n",
              "      <th>Input4</th>\n",
              "      <th>Input5</th>\n",
              "      <th>Input6</th>\n",
              "      <th>Input7</th>\n",
              "      <th>Input8</th>\n",
              "      <th>...</th>\n",
              "      <th>Input391</th>\n",
              "      <th>Input392</th>\n",
              "      <th>Input393</th>\n",
              "      <th>Input394</th>\n",
              "      <th>Input395</th>\n",
              "      <th>Input396</th>\n",
              "      <th>Input397</th>\n",
              "      <th>Input398</th>\n",
              "      <th>Input399</th>\n",
              "      <th>Input400</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.800586</td>\n",
              "      <td>-0.002583</td>\n",
              "      <td>2.184037</td>\n",
              "      <td>-0.322008</td>\n",
              "      <td>1.621241</td>\n",
              "      <td>1.192444</td>\n",
              "      <td>-0.278356</td>\n",
              "      <td>-0.207366</td>\n",
              "      <td>0.735689</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.140861</td>\n",
              "      <td>1.187660</td>\n",
              "      <td>0.345238</td>\n",
              "      <td>-0.844885</td>\n",
              "      <td>0.580007</td>\n",
              "      <td>-2.605781</td>\n",
              "      <td>-0.299471</td>\n",
              "      <td>0.711487</td>\n",
              "      <td>-0.753316</td>\n",
              "      <td>0.728763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2.168475</td>\n",
              "      <td>0.668637</td>\n",
              "      <td>1.373933</td>\n",
              "      <td>-0.476868</td>\n",
              "      <td>-0.724704</td>\n",
              "      <td>0.031162</td>\n",
              "      <td>-1.845921</td>\n",
              "      <td>0.784890</td>\n",
              "      <td>1.508526</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.286120</td>\n",
              "      <td>-0.900044</td>\n",
              "      <td>-0.500399</td>\n",
              "      <td>-0.126421</td>\n",
              "      <td>-0.632233</td>\n",
              "      <td>-2.557419</td>\n",
              "      <td>0.056044</td>\n",
              "      <td>0.634774</td>\n",
              "      <td>-0.259835</td>\n",
              "      <td>0.106390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.210777</td>\n",
              "      <td>-0.681438</td>\n",
              "      <td>-0.544753</td>\n",
              "      <td>0.441346</td>\n",
              "      <td>-0.019906</td>\n",
              "      <td>-0.192135</td>\n",
              "      <td>-0.162510</td>\n",
              "      <td>-0.998777</td>\n",
              "      <td>0.686472</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.391605</td>\n",
              "      <td>-0.190147</td>\n",
              "      <td>0.793746</td>\n",
              "      <td>-0.812737</td>\n",
              "      <td>-0.068228</td>\n",
              "      <td>-0.313143</td>\n",
              "      <td>2.564096</td>\n",
              "      <td>0.848355</td>\n",
              "      <td>0.180556</td>\n",
              "      <td>-1.525615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0.505678</td>\n",
              "      <td>-0.497957</td>\n",
              "      <td>0.720712</td>\n",
              "      <td>0.149120</td>\n",
              "      <td>0.019251</td>\n",
              "      <td>1.377850</td>\n",
              "      <td>0.981337</td>\n",
              "      <td>-0.846813</td>\n",
              "      <td>0.036790</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.176734</td>\n",
              "      <td>-0.947351</td>\n",
              "      <td>-0.888601</td>\n",
              "      <td>1.509450</td>\n",
              "      <td>-0.501929</td>\n",
              "      <td>-0.554909</td>\n",
              "      <td>-0.104051</td>\n",
              "      <td>0.442150</td>\n",
              "      <td>-0.056644</td>\n",
              "      <td>1.447267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>-10.281033</td>\n",
              "      <td>-1.178544</td>\n",
              "      <td>0.176941</td>\n",
              "      <td>1.112202</td>\n",
              "      <td>1.234189</td>\n",
              "      <td>0.999451</td>\n",
              "      <td>-0.773329</td>\n",
              "      <td>-0.811075</td>\n",
              "      <td>1.550537</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.181325</td>\n",
              "      <td>0.198960</td>\n",
              "      <td>-0.697497</td>\n",
              "      <td>-0.836371</td>\n",
              "      <td>1.652071</td>\n",
              "      <td>0.974292</td>\n",
              "      <td>1.584071</td>\n",
              "      <td>-0.202352</td>\n",
              "      <td>1.362426</td>\n",
              "      <td>1.023857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1</td>\n",
              "      <td>-4.298039</td>\n",
              "      <td>-0.893128</td>\n",
              "      <td>2.081556</td>\n",
              "      <td>0.796121</td>\n",
              "      <td>0.436108</td>\n",
              "      <td>-0.849635</td>\n",
              "      <td>1.129482</td>\n",
              "      <td>1.432813</td>\n",
              "      <td>-0.438694</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.474294</td>\n",
              "      <td>0.039143</td>\n",
              "      <td>1.808243</td>\n",
              "      <td>-0.034847</td>\n",
              "      <td>-1.314878</td>\n",
              "      <td>-1.235939</td>\n",
              "      <td>1.010456</td>\n",
              "      <td>-2.186403</td>\n",
              "      <td>-0.157829</td>\n",
              "      <td>0.738539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.431692</td>\n",
              "      <td>0.048336</td>\n",
              "      <td>0.770285</td>\n",
              "      <td>-0.354350</td>\n",
              "      <td>-1.557706</td>\n",
              "      <td>-0.182954</td>\n",
              "      <td>-0.665730</td>\n",
              "      <td>0.322526</td>\n",
              "      <td>0.658221</td>\n",
              "      <td>...</td>\n",
              "      <td>1.087032</td>\n",
              "      <td>0.095401</td>\n",
              "      <td>0.301200</td>\n",
              "      <td>1.776995</td>\n",
              "      <td>-2.045261</td>\n",
              "      <td>-1.931008</td>\n",
              "      <td>-0.683551</td>\n",
              "      <td>0.000835</td>\n",
              "      <td>-0.671151</td>\n",
              "      <td>-0.945843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.056681</td>\n",
              "      <td>1.404051</td>\n",
              "      <td>-0.061405</td>\n",
              "      <td>0.180448</td>\n",
              "      <td>-0.362992</td>\n",
              "      <td>0.826353</td>\n",
              "      <td>-0.066654</td>\n",
              "      <td>0.987946</td>\n",
              "      <td>-1.266302</td>\n",
              "      <td>...</td>\n",
              "      <td>1.011431</td>\n",
              "      <td>0.458901</td>\n",
              "      <td>-0.220498</td>\n",
              "      <td>0.004950</td>\n",
              "      <td>-1.928972</td>\n",
              "      <td>-1.574129</td>\n",
              "      <td>1.421012</td>\n",
              "      <td>-0.736559</td>\n",
              "      <td>-0.540174</td>\n",
              "      <td>-1.182067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.983396</td>\n",
              "      <td>-0.474238</td>\n",
              "      <td>-1.288631</td>\n",
              "      <td>-0.326170</td>\n",
              "      <td>-0.275383</td>\n",
              "      <td>-0.315331</td>\n",
              "      <td>-1.225622</td>\n",
              "      <td>-0.656750</td>\n",
              "      <td>0.777151</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.146012</td>\n",
              "      <td>-0.031477</td>\n",
              "      <td>-2.461869</td>\n",
              "      <td>1.037240</td>\n",
              "      <td>0.366076</td>\n",
              "      <td>-0.541171</td>\n",
              "      <td>-0.126733</td>\n",
              "      <td>-0.265069</td>\n",
              "      <td>-0.080381</td>\n",
              "      <td>0.166985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>0</td>\n",
              "      <td>-3.057179</td>\n",
              "      <td>1.535949</td>\n",
              "      <td>0.644094</td>\n",
              "      <td>-1.287768</td>\n",
              "      <td>-0.191350</td>\n",
              "      <td>0.167755</td>\n",
              "      <td>-0.534574</td>\n",
              "      <td>0.220055</td>\n",
              "      <td>-0.501486</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.201860</td>\n",
              "      <td>0.409375</td>\n",
              "      <td>-0.440580</td>\n",
              "      <td>1.040116</td>\n",
              "      <td>0.429276</td>\n",
              "      <td>-0.875186</td>\n",
              "      <td>-0.889635</td>\n",
              "      <td>0.315779</td>\n",
              "      <td>1.698054</td>\n",
              "      <td>0.903791</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 402 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ffeea00-14d7-4863-a0ee-6b47464979d6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3ffeea00-14d7-4863-a0ee-6b47464979d6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3ffeea00-14d7-4863-a0ee-6b47464979d6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d14a9251-4c2c-4365-96d7-ded4315e771f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d14a9251-4c2c-4365-96d7-ded4315e771f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d14a9251-4c2c-4365-96d7-ded4315e771f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_6047c253-b0c3-4f40-9eec-11039103180f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6047c253-b0c3-4f40-9eec-11039103180f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      Class     Output    Input1    Input2    Input3    Input4    Input5  \\\n",
              "0         0   0.800586 -0.002583  2.184037 -0.322008  1.621241  1.192444   \n",
              "1         0   2.168475  0.668637  1.373933 -0.476868 -0.724704  0.031162   \n",
              "2         1  -1.210777 -0.681438 -0.544753  0.441346 -0.019906 -0.192135   \n",
              "3         1   0.505678 -0.497957  0.720712  0.149120  0.019251  1.377850   \n",
              "4         1 -10.281033 -1.178544  0.176941  1.112202  1.234189  0.999451   \n",
              "...     ...        ...       ...       ...       ...       ...       ...   \n",
              "1995      1  -4.298039 -0.893128  2.081556  0.796121  0.436108 -0.849635   \n",
              "1996      1  -0.431692  0.048336  0.770285 -0.354350 -1.557706 -0.182954   \n",
              "1997      1  -0.056681  1.404051 -0.061405  0.180448 -0.362992  0.826353   \n",
              "1998      0  -0.983396 -0.474238 -1.288631 -0.326170 -0.275383 -0.315331   \n",
              "1999      0  -3.057179  1.535949  0.644094 -1.287768 -0.191350  0.167755   \n",
              "\n",
              "        Input6    Input7    Input8  ...  Input391  Input392  Input393  \\\n",
              "0    -0.278356 -0.207366  0.735689  ... -2.140861  1.187660  0.345238   \n",
              "1    -1.845921  0.784890  1.508526  ... -1.286120 -0.900044 -0.500399   \n",
              "2    -0.162510 -0.998777  0.686472  ... -0.391605 -0.190147  0.793746   \n",
              "3     0.981337 -0.846813  0.036790  ... -0.176734 -0.947351 -0.888601   \n",
              "4    -0.773329 -0.811075  1.550537  ... -0.181325  0.198960 -0.697497   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "1995  1.129482  1.432813 -0.438694  ... -0.474294  0.039143  1.808243   \n",
              "1996 -0.665730  0.322526  0.658221  ...  1.087032  0.095401  0.301200   \n",
              "1997 -0.066654  0.987946 -1.266302  ...  1.011431  0.458901 -0.220498   \n",
              "1998 -1.225622 -0.656750  0.777151  ... -1.146012 -0.031477 -2.461869   \n",
              "1999 -0.534574  0.220055 -0.501486  ... -1.201860  0.409375 -0.440580   \n",
              "\n",
              "      Input394  Input395  Input396  Input397  Input398  Input399  Input400  \n",
              "0    -0.844885  0.580007 -2.605781 -0.299471  0.711487 -0.753316  0.728763  \n",
              "1    -0.126421 -0.632233 -2.557419  0.056044  0.634774 -0.259835  0.106390  \n",
              "2    -0.812737 -0.068228 -0.313143  2.564096  0.848355  0.180556 -1.525615  \n",
              "3     1.509450 -0.501929 -0.554909 -0.104051  0.442150 -0.056644  1.447267  \n",
              "4    -0.836371  1.652071  0.974292  1.584071 -0.202352  1.362426  1.023857  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "1995 -0.034847 -1.314878 -1.235939  1.010456 -2.186403 -0.157829  0.738539  \n",
              "1996  1.776995 -2.045261 -1.931008 -0.683551  0.000835 -0.671151 -0.945843  \n",
              "1997  0.004950 -1.928972 -1.574129  1.421012 -0.736559 -0.540174 -1.182067  \n",
              "1998  1.037240  0.366076 -0.541171 -0.126733 -0.265069 -0.080381  0.166985  \n",
              "1999  1.040116  0.429276 -0.875186 -0.889635  0.315779  1.698054  0.903791  \n",
              "\n",
              "[2000 rows x 402 columns]"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('zad2_wum_data_for_students.csv', sep=';')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Bz77jUdNcaOo",
        "outputId": "f466ae18-15bd-4aae-8f36-65193046d167"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-5de95e97-e444-4c0e-b3b6-cd68b12f986f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Output</th>\n",
              "      <th>Input1</th>\n",
              "      <th>Input2</th>\n",
              "      <th>Input3</th>\n",
              "      <th>Input4</th>\n",
              "      <th>Input5</th>\n",
              "      <th>Input6</th>\n",
              "      <th>Input7</th>\n",
              "      <th>Input8</th>\n",
              "      <th>...</th>\n",
              "      <th>Input391</th>\n",
              "      <th>Input392</th>\n",
              "      <th>Input393</th>\n",
              "      <th>Input394</th>\n",
              "      <th>Input395</th>\n",
              "      <th>Input396</th>\n",
              "      <th>Input397</th>\n",
              "      <th>Input398</th>\n",
              "      <th>Input399</th>\n",
              "      <th>Input400</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.506500</td>\n",
              "      <td>0.106647</td>\n",
              "      <td>-0.005665</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.005334</td>\n",
              "      <td>0.003488</td>\n",
              "      <td>0.002534</td>\n",
              "      <td>0.006839</td>\n",
              "      <td>-0.012273</td>\n",
              "      <td>-0.020030</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.019331</td>\n",
              "      <td>0.029074</td>\n",
              "      <td>0.020895</td>\n",
              "      <td>-0.016766</td>\n",
              "      <td>-0.007556</td>\n",
              "      <td>0.010091</td>\n",
              "      <td>0.007851</td>\n",
              "      <td>0.012891</td>\n",
              "      <td>0.004650</td>\n",
              "      <td>-0.013988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.500083</td>\n",
              "      <td>3.562855</td>\n",
              "      <td>0.999429</td>\n",
              "      <td>1.010284</td>\n",
              "      <td>0.991542</td>\n",
              "      <td>0.987569</td>\n",
              "      <td>0.999472</td>\n",
              "      <td>1.008721</td>\n",
              "      <td>1.005626</td>\n",
              "      <td>1.002895</td>\n",
              "      <td>...</td>\n",
              "      <td>0.994439</td>\n",
              "      <td>1.000463</td>\n",
              "      <td>0.988078</td>\n",
              "      <td>0.994856</td>\n",
              "      <td>0.983238</td>\n",
              "      <td>0.996567</td>\n",
              "      <td>0.987002</td>\n",
              "      <td>0.995252</td>\n",
              "      <td>0.999906</td>\n",
              "      <td>0.997836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-12.384019</td>\n",
              "      <td>-4.040194</td>\n",
              "      <td>-3.119465</td>\n",
              "      <td>-2.978635</td>\n",
              "      <td>-3.667101</td>\n",
              "      <td>-3.466724</td>\n",
              "      <td>-3.230187</td>\n",
              "      <td>-3.686750</td>\n",
              "      <td>-2.971715</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.003626</td>\n",
              "      <td>-3.415436</td>\n",
              "      <td>-3.233725</td>\n",
              "      <td>-3.292675</td>\n",
              "      <td>-3.572882</td>\n",
              "      <td>-3.416226</td>\n",
              "      <td>-2.804361</td>\n",
              "      <td>-3.384484</td>\n",
              "      <td>-3.493723</td>\n",
              "      <td>-3.369950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.310265</td>\n",
              "      <td>-0.652010</td>\n",
              "      <td>-0.704739</td>\n",
              "      <td>-0.649305</td>\n",
              "      <td>-0.657961</td>\n",
              "      <td>-0.662386</td>\n",
              "      <td>-0.656393</td>\n",
              "      <td>-0.670924</td>\n",
              "      <td>-0.702121</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.682235</td>\n",
              "      <td>-0.637035</td>\n",
              "      <td>-0.644477</td>\n",
              "      <td>-0.687878</td>\n",
              "      <td>-0.648976</td>\n",
              "      <td>-0.720322</td>\n",
              "      <td>-0.670621</td>\n",
              "      <td>-0.644547</td>\n",
              "      <td>-0.665419</td>\n",
              "      <td>-0.690380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.140658</td>\n",
              "      <td>0.001009</td>\n",
              "      <td>-0.058297</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>-0.037171</td>\n",
              "      <td>0.020573</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>-0.006156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013317</td>\n",
              "      <td>0.027436</td>\n",
              "      <td>0.019631</td>\n",
              "      <td>-0.009518</td>\n",
              "      <td>-0.006203</td>\n",
              "      <td>0.051022</td>\n",
              "      <td>0.029848</td>\n",
              "      <td>0.042120</td>\n",
              "      <td>0.022766</td>\n",
              "      <td>-0.012325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.435991</td>\n",
              "      <td>0.639342</td>\n",
              "      <td>0.581323</td>\n",
              "      <td>0.676253</td>\n",
              "      <td>0.648669</td>\n",
              "      <td>0.684755</td>\n",
              "      <td>0.687058</td>\n",
              "      <td>0.664702</td>\n",
              "      <td>0.668752</td>\n",
              "      <td>...</td>\n",
              "      <td>0.642293</td>\n",
              "      <td>0.732498</td>\n",
              "      <td>0.679480</td>\n",
              "      <td>0.678636</td>\n",
              "      <td>0.659516</td>\n",
              "      <td>0.777529</td>\n",
              "      <td>0.660940</td>\n",
              "      <td>0.700344</td>\n",
              "      <td>0.680525</td>\n",
              "      <td>0.674120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.676146</td>\n",
              "      <td>3.370270</td>\n",
              "      <td>5.357014</td>\n",
              "      <td>3.674250</td>\n",
              "      <td>3.872662</td>\n",
              "      <td>3.507387</td>\n",
              "      <td>3.038254</td>\n",
              "      <td>3.764414</td>\n",
              "      <td>3.466016</td>\n",
              "      <td>...</td>\n",
              "      <td>3.601507</td>\n",
              "      <td>4.390463</td>\n",
              "      <td>3.150263</td>\n",
              "      <td>3.123106</td>\n",
              "      <td>3.294423</td>\n",
              "      <td>2.962170</td>\n",
              "      <td>3.069878</td>\n",
              "      <td>3.331561</td>\n",
              "      <td>3.054563</td>\n",
              "      <td>3.129274</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 402 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5de95e97-e444-4c0e-b3b6-cd68b12f986f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5de95e97-e444-4c0e-b3b6-cd68b12f986f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5de95e97-e444-4c0e-b3b6-cd68b12f986f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a7decff1-89d5-424a-82cf-da98d260e583\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7decff1-89d5-424a-82cf-da98d260e583')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a7decff1-89d5-424a-82cf-da98d260e583 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "             Class       Output       Input1       Input2       Input3  \\\n",
              "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
              "mean      0.506500     0.106647    -0.005665    -0.016681    -0.005334   \n",
              "std       0.500083     3.562855     0.999429     1.010284     0.991542   \n",
              "min       0.000000   -12.384019    -4.040194    -3.119465    -2.978635   \n",
              "25%       0.000000    -2.310265    -0.652010    -0.704739    -0.649305   \n",
              "50%       1.000000     0.140658     0.001009    -0.058297     0.025890   \n",
              "75%       1.000000     2.435991     0.639342     0.581323     0.676253   \n",
              "max       1.000000    11.676146     3.370270     5.357014     3.674250   \n",
              "\n",
              "            Input4       Input5       Input6       Input7       Input8  ...  \\\n",
              "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
              "mean      0.003488     0.002534     0.006839    -0.012273    -0.020030  ...   \n",
              "std       0.987569     0.999472     1.008721     1.005626     1.002895  ...   \n",
              "min      -3.667101    -3.466724    -3.230187    -3.686750    -2.971715  ...   \n",
              "25%      -0.657961    -0.662386    -0.656393    -0.670924    -0.702121  ...   \n",
              "50%       0.000425    -0.037171     0.020573     0.000160    -0.006156  ...   \n",
              "75%       0.648669     0.684755     0.687058     0.664702     0.668752  ...   \n",
              "max       3.872662     3.507387     3.038254     3.764414     3.466016  ...   \n",
              "\n",
              "          Input391     Input392     Input393     Input394     Input395  \\\n",
              "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
              "mean     -0.019331     0.029074     0.020895    -0.016766    -0.007556   \n",
              "std       0.994439     1.000463     0.988078     0.994856     0.983238   \n",
              "min      -4.003626    -3.415436    -3.233725    -3.292675    -3.572882   \n",
              "25%      -0.682235    -0.637035    -0.644477    -0.687878    -0.648976   \n",
              "50%       0.013317     0.027436     0.019631    -0.009518    -0.006203   \n",
              "75%       0.642293     0.732498     0.679480     0.678636     0.659516   \n",
              "max       3.601507     4.390463     3.150263     3.123106     3.294423   \n",
              "\n",
              "          Input396     Input397     Input398     Input399     Input400  \n",
              "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  \n",
              "mean      0.010091     0.007851     0.012891     0.004650    -0.013988  \n",
              "std       0.996567     0.987002     0.995252     0.999906     0.997836  \n",
              "min      -3.416226    -2.804361    -3.384484    -3.493723    -3.369950  \n",
              "25%      -0.720322    -0.670621    -0.644547    -0.665419    -0.690380  \n",
              "50%       0.051022     0.029848     0.042120     0.022766    -0.012325  \n",
              "75%       0.777529     0.660940     0.700344     0.680525     0.674120  \n",
              "max       2.962170     3.069878     3.331561     3.054563     3.129274  \n",
              "\n",
              "[8 rows x 402 columns]"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "Qr4fZrU0efYS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMNx0TTteOVZ"
      },
      "source": [
        "# Baseline models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_GPfxZHrc7W"
      },
      "source": [
        "### Regresja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L6GHNLgeQ9z",
        "outputId": "aa084499-e647-4e39-cba5-7271be1a410d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.615\n",
            "Model:                            OLS   Adj. R-squared:                  0.494\n",
            "Method:                 Least Squares   F-statistic:                     5.047\n",
            "Date:                Mon, 02 Jun 2025   Prob (F-statistic):          2.20e-103\n",
            "Time:                        18:45:40   Log-Likelihood:                -3538.3\n",
            "No. Observations:                1600   AIC:                             7849.\n",
            "Df Residuals:                    1214   BIC:                             9924.\n",
            "Df Model:                         385                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1446      0.067      2.144      0.032       0.012       0.277\n",
            "x1            -0.0236      0.073     -0.321      0.748      -0.168       0.120\n",
            "x2             0.0199      0.014      1.466      0.143      -0.007       0.047\n",
            "x3             0.0605      0.073      0.832      0.406      -0.082       0.203\n",
            "x4             0.0138      0.075      0.184      0.854      -0.133       0.161\n",
            "x5             0.0700      0.073      0.960      0.337      -0.073       0.213\n",
            "x6             0.0370      0.072      0.513      0.608      -0.105       0.179\n",
            "x7            -0.0029      0.073     -0.040      0.968      -0.146       0.140\n",
            "x8            -0.0159      0.074     -0.215      0.830      -0.161       0.129\n",
            "x9             0.1310      0.094      1.389      0.165      -0.054       0.316\n",
            "x10           -0.1193      0.071     -1.677      0.094      -0.259       0.020\n",
            "x11            0.1098      0.074      1.482      0.138      -0.036       0.255\n",
            "x12           -0.0390      0.075     -0.519      0.604      -0.187       0.108\n",
            "x13            0.1103      0.087      1.275      0.203      -0.059       0.280\n",
            "x14            0.0592      0.072      0.822      0.411      -0.082       0.201\n",
            "x15           -0.0067      0.074     -0.090      0.928      -0.152       0.139\n",
            "x16            0.0338      0.072      0.472      0.637      -0.107       0.175\n",
            "x17           -0.1469      0.088     -1.674      0.094      -0.319       0.025\n",
            "x18            0.4800      0.091      5.264      0.000       0.301       0.659\n",
            "x19            0.0391      0.072      0.542      0.588      -0.102       0.181\n",
            "x20            0.0310      0.074      0.419      0.675      -0.114       0.176\n",
            "x21            0.0779      0.073      1.066      0.287      -0.065       0.221\n",
            "x22            0.0240      0.072      0.334      0.738      -0.117       0.165\n",
            "x23           -0.0421      0.074     -0.572      0.568      -0.187       0.102\n",
            "x24            0.0069      0.074      0.093      0.926      -0.139       0.153\n",
            "x25            0.1407      0.072      1.947      0.052      -0.001       0.283\n",
            "x26            0.0602      0.074      0.813      0.416      -0.085       0.206\n",
            "x27            0.0482      0.070      0.690      0.490      -0.089       0.185\n",
            "x28            0.0392      0.073      0.535      0.593      -0.105       0.183\n",
            "x29           -0.0958      0.074     -1.301      0.194      -0.240       0.049\n",
            "x30            0.0435      0.072      0.605      0.545      -0.097       0.184\n",
            "x31           -0.0523      0.074     -0.709      0.478      -0.197       0.092\n",
            "x32           -0.0483      0.073     -0.659      0.510      -0.192       0.096\n",
            "x33           -0.0215      0.092     -0.235      0.814      -0.201       0.158\n",
            "x34           -0.0487      0.072     -0.675      0.500      -0.190       0.093\n",
            "x35            0.0349      0.072      0.486      0.627      -0.106       0.176\n",
            "x36           -0.0480      0.072     -0.669      0.503      -0.189       0.093\n",
            "x37            0.0040      0.075      0.054      0.957      -0.143       0.151\n",
            "x38            0.0147      0.019      0.785      0.433      -0.022       0.051\n",
            "x39            0.0685      0.073      0.937      0.349      -0.075       0.212\n",
            "x40           -0.0338      0.019     -1.790      0.074      -0.071       0.003\n",
            "x41            0.0425      0.021      2.005      0.045       0.001       0.084\n",
            "x42            0.0071      0.075      0.096      0.924      -0.140       0.154\n",
            "x43            0.0993      0.071      1.398      0.162      -0.040       0.239\n",
            "x44            0.1318      0.073      1.794      0.073      -0.012       0.276\n",
            "x45            0.0584      0.072      0.810      0.418      -0.083       0.200\n",
            "x46           -0.0089      0.074     -0.121      0.904      -0.153       0.136\n",
            "x47            0.0427      0.072      0.591      0.555      -0.099       0.185\n",
            "x48           -0.1177      0.075     -1.569      0.117      -0.265       0.030\n",
            "x49            0.0146      0.072      0.203      0.839      -0.126       0.156\n",
            "x50           -0.0609      0.073     -0.832      0.405      -0.205       0.083\n",
            "x51           -0.0147      0.073     -0.201      0.840      -0.158       0.128\n",
            "x52           -0.0492      0.076     -0.649      0.517      -0.198       0.100\n",
            "x53            0.0188      0.073      0.259      0.796      -0.124       0.161\n",
            "x54           -0.0511      0.071     -0.722      0.470      -0.190       0.088\n",
            "x55            0.0144      0.096      0.151      0.880      -0.173       0.202\n",
            "x56           -0.0109      0.072     -0.152      0.879      -0.152       0.130\n",
            "x57           -0.0080      0.101     -0.080      0.936      -0.205       0.189\n",
            "x58            0.0241      0.072      0.336      0.737      -0.117       0.165\n",
            "x59            0.4009      0.092      4.348      0.000       0.220       0.582\n",
            "x60           -0.0287      0.073     -0.394      0.694      -0.172       0.114\n",
            "x61            0.0284      0.071      0.397      0.691      -0.112       0.169\n",
            "x62           -0.1215      0.074     -1.651      0.099      -0.266       0.023\n",
            "x63           -0.0114      0.075     -0.152      0.879      -0.159       0.136\n",
            "x64            0.0714      0.073      0.977      0.329      -0.072       0.215\n",
            "x65            0.0355      0.018      1.996      0.046       0.001       0.070\n",
            "x66            0.0394      0.073      0.543      0.587      -0.103       0.182\n",
            "x67           -0.0964      0.072     -1.332      0.183      -0.238       0.046\n",
            "x68           -0.0967      0.072     -1.347      0.178      -0.237       0.044\n",
            "x69           -0.0811      0.073     -1.116      0.264      -0.224       0.061\n",
            "x70           -0.1013      0.073     -1.387      0.166      -0.245       0.042\n",
            "x71           -0.0131      0.076     -0.173      0.863      -0.162       0.136\n",
            "x72            0.0088      0.075      0.117      0.907      -0.138       0.156\n",
            "x73           -0.0681      0.073     -0.932      0.351      -0.211       0.075\n",
            "x74            0.0085      0.018      0.464      0.643      -0.027       0.044\n",
            "x75            0.1262      0.074      1.715      0.087      -0.018       0.271\n",
            "x76            0.0368      0.071      0.519      0.604      -0.102       0.176\n",
            "x77           -0.0113      0.074     -0.153      0.878      -0.156       0.134\n",
            "x78            0.1823      0.088      2.066      0.039       0.009       0.355\n",
            "x79           -0.0513      0.072     -0.712      0.477      -0.193       0.090\n",
            "x80           -0.0788      0.071     -1.102      0.271      -0.219       0.061\n",
            "x81            0.0814      0.071      1.144      0.253      -0.058       0.221\n",
            "x82           -0.0529      0.073     -0.721      0.471      -0.197       0.091\n",
            "x83            0.8848      0.096      9.244      0.000       0.697       1.073\n",
            "x84            0.0056      0.073      0.077      0.939      -0.138       0.149\n",
            "x85            0.0478      0.074      0.647      0.518      -0.097       0.193\n",
            "x86           -0.0459      0.072     -0.635      0.525      -0.188       0.096\n",
            "x87           -0.0812      0.074     -1.105      0.269      -0.225       0.063\n",
            "x88           -0.0992      0.073     -1.351      0.177      -0.243       0.045\n",
            "x89           -0.0008      0.072     -0.010      0.992      -0.142       0.140\n",
            "x90           -0.0585      0.072     -0.813      0.416      -0.200       0.083\n",
            "x91            0.0926      0.096      0.970      0.332      -0.095       0.280\n",
            "x92            0.1144      0.073      1.569      0.117      -0.029       0.257\n",
            "x93           -0.1089      0.072     -1.515      0.130      -0.250       0.032\n",
            "x94            0.1380      0.072      1.925      0.054      -0.003       0.279\n",
            "x95            0.0399      0.021      1.880      0.060      -0.002       0.082\n",
            "x96           -0.1165      0.075     -1.555      0.120      -0.263       0.030\n",
            "x97           -0.0009      0.075     -0.012      0.990      -0.148       0.146\n",
            "x98            0.1456      0.072      2.031      0.043       0.005       0.286\n",
            "x99           -0.0190      0.094     -0.203      0.839      -0.203       0.165\n",
            "x100          -0.1318      0.074     -1.783      0.075      -0.277       0.013\n",
            "x101          -0.0076      0.017     -0.451      0.652      -0.041       0.025\n",
            "x102           0.0777      0.074      1.045      0.296      -0.068       0.224\n",
            "x103           0.0242      0.073      0.333      0.739      -0.118       0.167\n",
            "x104          -0.0964      0.074     -1.298      0.195      -0.242       0.049\n",
            "x105          -0.0736      0.072     -1.024      0.306      -0.215       0.067\n",
            "x106           0.0009      0.072      0.013      0.990      -0.141       0.143\n",
            "x107           0.0080      0.074      0.109      0.913      -0.137       0.153\n",
            "x108          -0.0764      0.075     -1.020      0.308      -0.223       0.071\n",
            "x109           0.0599      0.072      0.833      0.405      -0.081       0.201\n",
            "x110           0.0180      0.025      0.720      0.472      -0.031       0.067\n",
            "x111           0.0408      0.074      0.551      0.582      -0.105       0.186\n",
            "x112          -0.1023      0.070     -1.451      0.147      -0.241       0.036\n",
            "x113           0.0819      0.071      1.158      0.247      -0.057       0.221\n",
            "x114           0.0554      0.074      0.747      0.455      -0.090       0.201\n",
            "x115           0.0267      0.072      0.372      0.710      -0.114       0.168\n",
            "x116           0.0249      0.072      0.344      0.731      -0.117       0.167\n",
            "x117           0.0358      0.072      0.501      0.616      -0.104       0.176\n",
            "x118           0.0681      0.072      0.940      0.347      -0.074       0.210\n",
            "x119           0.0909      0.072      1.258      0.209      -0.051       0.233\n",
            "x120          -0.0260      0.074     -0.349      0.727      -0.172       0.120\n",
            "x121          -0.1109      0.073     -1.528      0.127      -0.253       0.031\n",
            "x122           0.0103      0.074      0.140      0.889      -0.134       0.155\n",
            "x123          -0.1558      0.072     -2.162      0.031      -0.297      -0.014\n",
            "x124           0.0262      0.074      0.353      0.724      -0.119       0.172\n",
            "x125          -0.0006      0.074     -0.008      0.994      -0.146       0.145\n",
            "x126          -0.0983      0.072     -1.362      0.174      -0.240       0.043\n",
            "x127          -0.0514      0.072     -0.709      0.478      -0.194       0.091\n",
            "x128           0.0486      0.073      0.661      0.509      -0.096       0.193\n",
            "x129          -0.0789      0.073     -1.084      0.278      -0.222       0.064\n",
            "x130          -0.0086      0.075     -0.115      0.908      -0.156       0.139\n",
            "x131          -0.1601      0.072     -2.227      0.026      -0.301      -0.019\n",
            "x132           0.0469      0.072      0.652      0.514      -0.094       0.188\n",
            "x133           0.0301      0.074      0.409      0.683      -0.114       0.175\n",
            "x134           0.0035      0.073      0.049      0.961      -0.139       0.146\n",
            "x135          -0.0302      0.072     -0.419      0.675      -0.172       0.111\n",
            "x136           0.5614      0.090      6.203      0.000       0.384       0.739\n",
            "x137           0.0066      0.015      0.446      0.656      -0.022       0.036\n",
            "x138           0.0332      0.072      0.459      0.646      -0.109       0.175\n",
            "x139           0.0960      0.074      1.296      0.195      -0.049       0.241\n",
            "x140           0.0807      0.075      1.079      0.281      -0.066       0.227\n",
            "x141           0.0241      0.072      0.333      0.739      -0.118       0.166\n",
            "x142           0.0624      0.072      0.865      0.387      -0.079       0.204\n",
            "x143          -0.0031      0.094     -0.033      0.974      -0.187       0.181\n",
            "x144          -0.0308      0.075     -0.411      0.681      -0.178       0.116\n",
            "x145           0.0669      0.071      0.937      0.349      -0.073       0.207\n",
            "x146          -0.0263      0.074     -0.354      0.724      -0.172       0.119\n",
            "x147           0.0376      0.072      0.519      0.604      -0.104       0.179\n",
            "x148           0.0554      0.073      0.757      0.449      -0.088       0.199\n",
            "x149          -0.0962      0.074     -1.300      0.194      -0.241       0.049\n",
            "x150          -0.0268      0.019     -1.440      0.150      -0.063       0.010\n",
            "x151          -0.0773      0.074     -1.049      0.294      -0.222       0.067\n",
            "x152          -0.0882      0.072     -1.231      0.219      -0.229       0.052\n",
            "x153          -0.0394      0.099     -0.400      0.689      -0.233       0.154\n",
            "x154           0.0197      0.074      0.267      0.789      -0.125       0.164\n",
            "x155          -0.0126      0.071     -0.177      0.860      -0.152       0.127\n",
            "x156           0.1151      0.072      1.588      0.113      -0.027       0.257\n",
            "x157          -0.0090      0.072     -0.124      0.901      -0.151       0.133\n",
            "x158          -0.0918      0.073     -1.259      0.208      -0.235       0.051\n",
            "x159           0.0620      0.074      0.842      0.400      -0.083       0.207\n",
            "x160           0.0696      0.074      0.944      0.345      -0.075       0.214\n",
            "x161          -0.0550      0.071     -0.773      0.440      -0.195       0.085\n",
            "x162          -0.0143      0.073     -0.194      0.846      -0.158       0.130\n",
            "x163           0.0207      0.073      0.284      0.777      -0.122       0.164\n",
            "x164          -0.0874      0.073     -1.194      0.233      -0.231       0.056\n",
            "x165          -0.1200      0.074     -1.631      0.103      -0.264       0.024\n",
            "x166           0.1050      0.073      1.439      0.150      -0.038       0.248\n",
            "x167           0.9049      0.096      9.441      0.000       0.717       1.093\n",
            "x168          -0.0170      0.073     -0.231      0.817      -0.161       0.127\n",
            "x169          -0.0977      0.072     -1.351      0.177      -0.240       0.044\n",
            "x170           0.0672      0.072      0.934      0.351      -0.074       0.208\n",
            "x171           0.0687      0.072      0.957      0.339      -0.072       0.210\n",
            "x172          -0.1341      0.072     -1.855      0.064      -0.276       0.008\n",
            "x173           0.5539      0.090      6.181      0.000       0.378       0.730\n",
            "x174           0.0709      0.074      0.963      0.336      -0.074       0.215\n",
            "x175          -0.0579      0.073     -0.789      0.430      -0.202       0.086\n",
            "x176           0.1100      0.074      1.481      0.139      -0.036       0.256\n",
            "x177           0.0056      0.073      0.076      0.939      -0.138       0.149\n",
            "x178          -0.0046      0.073     -0.063      0.950      -0.147       0.138\n",
            "x179          -0.0195      0.091     -0.214      0.830      -0.199       0.160\n",
            "x180           0.1053      0.073      1.451      0.147      -0.037       0.248\n",
            "x181          -0.0130      0.074     -0.177      0.859      -0.158       0.131\n",
            "x182           0.0403      0.072      0.560      0.575      -0.101       0.181\n",
            "x183          -0.0754      0.073     -1.027      0.305      -0.220       0.069\n",
            "x184           0.7134      0.090      7.932      0.000       0.537       0.890\n",
            "x185           0.1161      0.082      1.424      0.155      -0.044       0.276\n",
            "x186           0.1390      0.075      1.859      0.063      -0.008       0.286\n",
            "x187           0.0403      0.073      0.549      0.583      -0.104       0.184\n",
            "x188          -0.0109      0.073     -0.150      0.881      -0.154       0.132\n",
            "x189           0.0214      0.071      0.303      0.762      -0.117       0.160\n",
            "x190          -0.0075      0.074     -0.102      0.919      -0.152       0.137\n",
            "x191          -0.0887      0.074     -1.193      0.233      -0.235       0.057\n",
            "x192          -0.0479      0.074     -0.650      0.516      -0.192       0.097\n",
            "x193           0.6689      0.093      7.229      0.000       0.487       0.850\n",
            "x194           0.1245      0.073      1.714      0.087      -0.018       0.267\n",
            "x195          -0.0752      0.073     -1.023      0.306      -0.219       0.069\n",
            "x196          -0.0679      0.073     -0.936      0.349      -0.210       0.074\n",
            "x197          -0.0629      0.073     -0.856      0.392      -0.207       0.081\n",
            "x198           0.0105      0.076      0.138      0.890      -0.139       0.160\n",
            "x199          -0.0580      0.073     -0.796      0.426      -0.201       0.085\n",
            "x200           0.0240      0.072      0.336      0.737      -0.116       0.164\n",
            "x201           0.0393      0.073      0.538      0.590      -0.104       0.182\n",
            "x202           0.0507      0.071      0.717      0.473      -0.088       0.189\n",
            "x203          -0.0533      0.071     -0.753      0.452      -0.192       0.086\n",
            "x204           0.1375      0.099      1.386      0.166      -0.057       0.332\n",
            "x205          -0.1293      0.072     -1.800      0.072      -0.270       0.012\n",
            "x206          -0.0092      0.018     -0.513      0.608      -0.044       0.026\n",
            "x207          -0.0947      0.090     -1.052      0.293      -0.271       0.082\n",
            "x208          -0.1205      0.073     -1.657      0.098      -0.263       0.022\n",
            "x209          -0.0259      0.072     -0.361      0.718      -0.167       0.115\n",
            "x210           0.0620      0.075      0.831      0.406      -0.084       0.208\n",
            "x211           0.0265      0.073      0.361      0.718      -0.117       0.170\n",
            "x212           0.0298      0.089      0.336      0.737      -0.144       0.204\n",
            "x213           0.1149      0.074      1.552      0.121      -0.030       0.260\n",
            "x214          -0.0308      0.072     -0.426      0.670      -0.172       0.111\n",
            "x215          -0.0418      0.071     -0.587      0.558      -0.181       0.098\n",
            "x216          -0.0203      0.074     -0.273      0.785      -0.166       0.126\n",
            "x217           0.0119      0.087      0.136      0.892      -0.159       0.183\n",
            "x218           0.0234      0.073      0.320      0.749      -0.120       0.167\n",
            "x219           0.0151      0.072      0.209      0.835      -0.127       0.157\n",
            "x220          -0.0851      0.075     -1.129      0.259      -0.233       0.063\n",
            "x221           0.0460      0.075      0.613      0.540      -0.101       0.193\n",
            "x222          -0.0897      0.094     -0.949      0.343      -0.275       0.096\n",
            "x223           0.9898      0.096     10.351      0.000       0.802       1.177\n",
            "x224          -0.0577      0.074     -0.779      0.436      -0.203       0.088\n",
            "x225           0.0063      0.072      0.087      0.931      -0.136       0.148\n",
            "x226           0.1157      0.073      1.590      0.112      -0.027       0.258\n",
            "x227          -0.0480      0.072     -0.670      0.503      -0.188       0.092\n",
            "x228           0.0211      0.072      0.291      0.771      -0.121       0.163\n",
            "x229          -0.0479      0.073     -0.661      0.509      -0.190       0.094\n",
            "x230           0.0156      0.076      0.206      0.837      -0.133       0.164\n",
            "x231          -0.1610      0.073     -2.201      0.028      -0.305      -0.017\n",
            "x232          -0.1005      0.073     -1.369      0.171      -0.245       0.044\n",
            "x233          -0.0309      0.073     -0.423      0.672      -0.174       0.113\n",
            "x234          -0.0898      0.073     -1.223      0.222      -0.234       0.054\n",
            "x235          -0.0325      0.074     -0.440      0.660      -0.177       0.112\n",
            "x236           0.1336      0.073      1.821      0.069      -0.010       0.277\n",
            "x237           0.0354      0.073      0.486      0.627      -0.108       0.179\n",
            "x238           0.0068      0.015      0.445      0.656      -0.023       0.037\n",
            "x239           0.0240      0.074      0.323      0.746      -0.121       0.169\n",
            "x240          -0.0313      0.019     -1.640      0.101      -0.069       0.006\n",
            "x241           0.4556      0.099      4.625      0.000       0.262       0.649\n",
            "x242           0.1012      0.071      1.417      0.157      -0.039       0.241\n",
            "x243          -0.0024      0.072     -0.033      0.973      -0.144       0.139\n",
            "x244          -0.1103      0.072     -1.529      0.127      -0.252       0.031\n",
            "x245           0.0889      0.075      1.184      0.237      -0.058       0.236\n",
            "x246          -0.0314      0.021     -1.489      0.137      -0.073       0.010\n",
            "x247           0.0440      0.071      0.616      0.538      -0.096       0.184\n",
            "x248          -0.0776      0.073     -1.065      0.287      -0.220       0.065\n",
            "x249           0.0110      0.070      0.157      0.875      -0.127       0.149\n",
            "x250           0.0913      0.094      0.968      0.333      -0.094       0.276\n",
            "x251           0.0846      0.072      1.170      0.242      -0.057       0.226\n",
            "x252           0.0565      0.074      0.763      0.446      -0.089       0.202\n",
            "x253          -0.0528      0.072     -0.728      0.467      -0.195       0.089\n",
            "x254           0.1039      0.072      1.445      0.149      -0.037       0.245\n",
            "x255          -0.0527      0.073     -0.721      0.471      -0.196       0.091\n",
            "x256           0.0106      0.024      0.448      0.654      -0.036       0.057\n",
            "x257           0.0278      0.073      0.383      0.702      -0.115       0.170\n",
            "x258          -0.0216      0.095     -0.226      0.821      -0.209       0.166\n",
            "x259           0.0359      0.072      0.495      0.620      -0.106       0.178\n",
            "x260          -0.1009      0.073     -1.373      0.170      -0.245       0.043\n",
            "x261          -0.0499      0.073     -0.680      0.496      -0.194       0.094\n",
            "x262           0.0723      0.072      1.001      0.317      -0.069       0.214\n",
            "x263           0.1198      0.087      1.370      0.171      -0.052       0.291\n",
            "x264           0.0582      0.072      0.803      0.422      -0.084       0.200\n",
            "x265           0.0996      0.073      1.364      0.173      -0.044       0.243\n",
            "x266           0.0792      0.074      1.076      0.282      -0.065       0.224\n",
            "x267           0.0796      0.071      1.114      0.265      -0.061       0.220\n",
            "x268          -0.0118      0.073     -0.163      0.871      -0.154       0.130\n",
            "x269          -0.0338      0.072     -0.471      0.637      -0.174       0.107\n",
            "x270          -0.1478      0.073     -2.015      0.044      -0.292      -0.004\n",
            "x271          -0.0264      0.074     -0.358      0.721      -0.171       0.118\n",
            "x272           0.0305      0.072      0.425      0.671      -0.110       0.171\n",
            "x273          -0.0253      0.071     -0.356      0.722      -0.165       0.114\n",
            "x274           0.0474      0.073      0.650      0.516      -0.096       0.190\n",
            "x275          -0.0092      0.072     -0.128      0.898      -0.150       0.132\n",
            "x276          -0.0845      0.071     -1.191      0.234      -0.224       0.055\n",
            "x277           0.0392      0.070      0.563      0.574      -0.097       0.176\n",
            "x278           0.0038      0.072      0.053      0.958      -0.138       0.146\n",
            "x279           0.1003      0.073      1.369      0.171      -0.043       0.244\n",
            "x280          -0.0052      0.072     -0.072      0.942      -0.147       0.136\n",
            "x281          -0.1534      0.074     -2.079      0.038      -0.298      -0.009\n",
            "x282          -0.0369      0.074     -0.499      0.618      -0.182       0.108\n",
            "x283          -0.0143      0.072     -0.199      0.842      -0.155       0.127\n",
            "x284          -0.0500      0.073     -0.689      0.491      -0.192       0.092\n",
            "x285          -0.1630      0.071     -2.285      0.023      -0.303      -0.023\n",
            "x286           0.1537      0.092      1.678      0.094      -0.026       0.334\n",
            "x287           0.0512      0.073      0.699      0.485      -0.093       0.195\n",
            "x288          -0.0306      0.074     -0.414      0.679      -0.176       0.115\n",
            "x289          -0.0076      0.071     -0.108      0.914      -0.147       0.131\n",
            "x290           0.0307      0.074      0.417      0.677      -0.114       0.175\n",
            "x291           0.0014      0.076      0.019      0.985      -0.148       0.150\n",
            "x292           0.7916      0.087      9.081      0.000       0.621       0.963\n",
            "x293           0.0331      0.018      1.838      0.066      -0.002       0.068\n",
            "x294           0.0222      0.093      0.240      0.811      -0.159       0.204\n",
            "x295           0.0829      0.091      0.910      0.363      -0.096       0.262\n",
            "x296           0.0564      0.074      0.766      0.444      -0.088       0.201\n",
            "x297           0.0062      0.073      0.085      0.932      -0.138       0.150\n",
            "x298          -0.0481      0.087     -0.556      0.579      -0.218       0.122\n",
            "x299           0.0011      0.073      0.015      0.988      -0.142       0.144\n",
            "x300          -0.1491      0.072     -2.065      0.039      -0.291      -0.007\n",
            "x301          -0.1440      0.099     -1.454      0.146      -0.338       0.050\n",
            "x302          -0.0099      0.090     -0.111      0.912      -0.186       0.166\n",
            "x303           0.0720      0.073      0.984      0.325      -0.072       0.215\n",
            "x304           0.1745      0.071      2.461      0.014       0.035       0.314\n",
            "x305          -0.0203      0.072     -0.283      0.777      -0.161       0.120\n",
            "x306           0.0816      0.075      1.089      0.276      -0.065       0.229\n",
            "x307          -0.0419      0.075     -0.560      0.576      -0.189       0.105\n",
            "x308           0.0103      0.017      0.619      0.536      -0.022       0.043\n",
            "x309           0.0005      0.075      0.007      0.994      -0.146       0.147\n",
            "x310          -0.0699      0.073     -0.954      0.340      -0.214       0.074\n",
            "x311           0.0309      0.073      0.423      0.672      -0.112       0.174\n",
            "x312           0.0673      0.073      0.923      0.356      -0.076       0.210\n",
            "x313           0.0360      0.075      0.480      0.631      -0.111       0.183\n",
            "x314          -0.0107      0.072     -0.149      0.882      -0.152       0.131\n",
            "x315          -0.0356      0.073     -0.486      0.627      -0.179       0.108\n",
            "x316          -0.0523      0.073     -0.718      0.473      -0.195       0.091\n",
            "x317          -0.0040      0.073     -0.055      0.956      -0.148       0.140\n",
            "x318           0.0307      0.075      0.411      0.681      -0.116       0.177\n",
            "x319          -0.0426      0.072     -0.595      0.552      -0.183       0.098\n",
            "x320          -0.0522      0.074     -0.710      0.478      -0.197       0.092\n",
            "x321           0.0790      0.073      1.080      0.281      -0.065       0.223\n",
            "x322          -0.1087      0.074     -1.462      0.144      -0.255       0.037\n",
            "x323          -0.0701      0.073     -0.963      0.336      -0.213       0.073\n",
            "x324          -0.1461      0.073     -2.010      0.045      -0.289      -0.004\n",
            "x325           0.0341      0.073      0.470      0.638      -0.108       0.177\n",
            "x326          -0.0266      0.073     -0.363      0.716      -0.170       0.117\n",
            "x327           0.0308      0.072      0.424      0.671      -0.111       0.173\n",
            "x328           0.0257      0.072      0.357      0.721      -0.116       0.167\n",
            "x329           0.0336      0.072      0.466      0.641      -0.108       0.175\n",
            "x330           0.0195      0.016      1.224      0.221      -0.012       0.051\n",
            "x331          -0.0774      0.072     -1.071      0.285      -0.219       0.064\n",
            "x332           0.0246      0.073      0.338      0.735      -0.118       0.167\n",
            "x333          -0.0508      0.073     -0.694      0.488      -0.194       0.093\n",
            "x334           0.0155      0.074      0.209      0.834      -0.130       0.161\n",
            "x335           0.1013      0.071      1.432      0.153      -0.038       0.240\n",
            "x336           0.0192      0.075      0.256      0.798      -0.128       0.166\n",
            "x337          -0.0572      0.073     -0.788      0.431      -0.199       0.085\n",
            "x338          -0.0343      0.071     -0.481      0.630      -0.174       0.106\n",
            "x339          -0.0324      0.073     -0.445      0.656      -0.175       0.110\n",
            "x340          -0.0098      0.073     -0.134      0.893      -0.152       0.133\n",
            "x341           0.0275      0.072      0.381      0.703      -0.114       0.169\n",
            "x342           0.8866      0.097      9.150      0.000       0.696       1.077\n",
            "x343           0.0225      0.074      0.304      0.761      -0.122       0.167\n",
            "x344           0.0721      0.072      0.999      0.318      -0.070       0.214\n",
            "x345          -0.0761      0.073     -1.048      0.295      -0.218       0.066\n",
            "x346          -0.0336      0.073     -0.459      0.646      -0.177       0.110\n",
            "x347           0.1473      0.072      2.047      0.041       0.006       0.289\n",
            "x348          -0.0477      0.072     -0.664      0.507      -0.189       0.093\n",
            "x349          -0.0370      0.073     -0.504      0.614      -0.181       0.107\n",
            "x350           0.0125      0.072      0.174      0.862      -0.128       0.153\n",
            "x351           0.1333      0.073      1.814      0.070      -0.011       0.277\n",
            "x352          -0.0127      0.073     -0.174      0.862      -0.156       0.131\n",
            "x353           0.0444      0.072      0.615      0.538      -0.097       0.186\n",
            "x354          -0.0104      0.074     -0.140      0.889      -0.156       0.135\n",
            "x355           0.1093      0.074      1.470      0.142      -0.037       0.255\n",
            "x356           0.0356      0.072      0.495      0.621      -0.106       0.177\n",
            "x357          -0.0146      0.072     -0.204      0.838      -0.155       0.126\n",
            "x358          -0.0055      0.073     -0.075      0.940      -0.148       0.137\n",
            "x359           0.0391      0.095      0.413      0.680      -0.147       0.225\n",
            "x360           0.0441      0.070      0.629      0.530      -0.094       0.182\n",
            "x361           0.0815      0.070      1.171      0.242      -0.055       0.218\n",
            "x362          -0.1285      0.073     -1.757      0.079      -0.272       0.015\n",
            "x363           0.0635      0.075      0.851      0.395      -0.083       0.210\n",
            "x364          -0.0069      0.071     -0.097      0.923      -0.146       0.132\n",
            "x365           0.0305      0.072      0.425      0.671      -0.110       0.171\n",
            "x366           0.0436      0.073      0.596      0.551      -0.100       0.187\n",
            "x367          -0.0251      0.074     -0.340      0.734      -0.170       0.120\n",
            "x368           0.1580      0.099      1.604      0.109      -0.035       0.351\n",
            "x369          -0.0007      0.075     -0.010      0.992      -0.147       0.146\n",
            "x370           0.0012      0.073      0.016      0.987      -0.142       0.144\n",
            "x371          -0.0392      0.076     -0.519      0.604      -0.188       0.109\n",
            "x372          -0.0626      0.072     -0.867      0.386      -0.204       0.079\n",
            "x373          -0.0309      0.074     -0.417      0.677      -0.177       0.115\n",
            "x374          -0.0051      0.072     -0.070      0.944      -0.147       0.137\n",
            "x375          -0.0199      0.072     -0.275      0.783      -0.162       0.122\n",
            "x376          -0.0419      0.094     -0.443      0.658      -0.227       0.143\n",
            "x377          -0.0312      0.086     -0.361      0.718      -0.201       0.138\n",
            "x378           0.0062      0.074      0.084      0.933      -0.139       0.152\n",
            "x379          -0.0002      0.095     -0.002      0.998      -0.187       0.186\n",
            "x380          -0.0520      0.074     -0.707      0.480      -0.196       0.092\n",
            "x381          -0.0363      0.073     -0.496      0.620      -0.180       0.107\n",
            "x382          -0.0390      0.073     -0.537      0.591      -0.181       0.103\n",
            "x383           0.0356      0.075      0.475      0.635      -0.111       0.182\n",
            "x384           0.0186      0.096      0.193      0.847      -0.171       0.208\n",
            "x385          -0.0481      0.094     -0.509      0.611      -0.233       0.137\n",
            "x386          -0.0341      0.075     -0.453      0.650      -0.182       0.114\n",
            "x387           0.5981      0.090      6.641      0.000       0.421       0.775\n",
            "x388          -0.0166      0.100     -0.167      0.868      -0.212       0.179\n",
            "x389           0.2969      0.089      3.328      0.001       0.122       0.472\n",
            "x390          -0.0647      0.072     -0.897      0.370      -0.206       0.077\n",
            "x391          -0.1282      0.100     -1.286      0.199      -0.324       0.067\n",
            "x392          -0.0150      0.072     -0.207      0.836      -0.157       0.127\n",
            "x393           0.0821      0.074      1.108      0.268      -0.063       0.228\n",
            "x394           0.0827      0.072      1.150      0.250      -0.058       0.224\n",
            "x395          -0.0473      0.073     -0.644      0.520      -0.192       0.097\n",
            "x396           0.0047      0.021      0.230      0.818      -0.036       0.045\n",
            "x397          -0.0630      0.074     -0.854      0.393      -0.208       0.082\n",
            "x398           0.0245      0.072      0.342      0.733      -0.116       0.165\n",
            "x399           0.0145      0.073      0.200      0.841      -0.128       0.157\n",
            "x400          -0.0558      0.074     -0.756      0.450      -0.200       0.089\n",
            "==============================================================================\n",
            "Omnibus:                        0.789   Durbin-Watson:                   2.016\n",
            "Prob(Omnibus):                  0.674   Jarque-Bera (JB):                0.702\n",
            "Skew:                           0.043   Prob(JB):                        0.704\n",
            "Kurtosis:                       3.057   Cond. No.                     2.09e+15\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 2.19e-27. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n",
            "\n",
            "Measures for training set\n",
            "(MSE) Mean squared error: 4.879\n",
            "(RMSE) Root mean squared error: 2.209\n",
            "\n",
            "Measures for test set\n",
            "(MSE) Mean squared error: 7.008\n",
            "(RMSE) Root mean squared error: 2.647\n",
            "Difference between test and train MSE: 2.129\n",
            "Difference between test and train RMSE: 0.438\n",
            "Training R^2: 0.615\n",
            "Test R^2: 0.446\n"
          ]
        }
      ],
      "source": [
        "def base_regression(df):\n",
        "  X = np.array(df.drop(['Class', 'Output'], axis=1))\n",
        "  Y = np.array(df[['Output']])\n",
        "\n",
        "  xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "  xtrain_const = sm.add_constant(xtrain)\n",
        "  lr = sm.OLS(ytrain, xtrain_const)\n",
        "  results = lr.fit()\n",
        "  print(results.summary())\n",
        "\n",
        "  ypred_train = results.predict(xtrain_const)\n",
        "  train_mse = mean_squared_error(ytrain, ypred_train)\n",
        "  train_rmse =  np.sqrt(mean_squared_error(ytrain, ypred_train))\n",
        "  print(\"\\nMeasures for training set\")\n",
        "  print(f\"(MSE) Mean squared error: {train_mse:.3f}\")\n",
        "  print(f\"(RMSE) Root mean squared error: {train_rmse:.3f}\")\n",
        "\n",
        "  xtest_const = sm.add_constant(xtest)\n",
        "  ypred_test = results.predict(xtest_const)\n",
        "  test_mse = mean_squared_error(ytest, ypred_test)\n",
        "  test_rmse =  np.sqrt(mean_squared_error(ytest, ypred_test))\n",
        "  print(\"\\nMeasures for test set\")\n",
        "  print(f\"(MSE) Mean squared error: {test_mse:.3f}\")\n",
        "  print(f\"(RMSE) Root mean squared error: {test_rmse:.3f}\")\n",
        "\n",
        "  print(f\"Difference between test and train MSE: {(test_mse - train_mse):.3f}\")\n",
        "  print(f\"Difference between test and train RMSE: {(test_rmse - train_rmse):.3f}\")\n",
        "\n",
        "  print(f\"Training R^2: {r2_score(ytrain, ypred_train):.3f}\")\n",
        "  print(f\"Test R^2: {r2_score(ytest, ypred_test):.3f}\")\n",
        "\n",
        "base_regression(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mHuk9JRqeDm"
      },
      "source": [
        "Widzimy, że różnica w `RMSE` między zbiorem treningowym, a testowym to ok. `1.9%` zakresu `Y`, więc to dużo nie jest, ale `R^2` w zbiorze treningowym jest trochę wyższa niż dla zbioru testowego, co może świadczyć o tym, że model nie generalizuje do końca dobrze. Gorzej radzi sobie z danymi testowymi. To znaczy, że mamy tu delikatny **overfitting**.\n",
        "\n",
        "Występuje tu także **underfitting**, ponieważ `R^2` nie jest duże na zbiorze treningowym."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7TuDe_Mris_"
      },
      "source": [
        "### Klasyfikacja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb9yJMuYrka-",
        "outputId": "94bb3acd-808b-41de-e677-162a9fc9155a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Measures for training set\n",
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.72      0.73       785\n",
            "           1       0.74      0.75      0.75       815\n",
            "\n",
            "    accuracy                           0.74      1600\n",
            "   macro avg       0.74      0.74      0.74      1600\n",
            "weighted avg       0.74      0.74      0.74      1600\n",
            "\n",
            "\n",
            "Measures for test set\n",
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.47      0.49       202\n",
            "           1       0.51      0.57      0.54       198\n",
            "\n",
            "    accuracy                           0.52       400\n",
            "   macro avg       0.52      0.52      0.52       400\n",
            "weighted avg       0.52      0.52      0.52       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def base_classification(df):\n",
        "  X = np.array(df.drop(['Class', 'Output'], axis=1))\n",
        "  Y = np.array(df[['Class']])\n",
        "\n",
        "  xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "  lr = LogisticRegression().fit(xtrain, ytrain)\n",
        "\n",
        "  ypred_train = lr.predict(xtrain)\n",
        "  ypred_test = lr.predict(xtest)\n",
        "\n",
        "  print(\"Measures for training set\")\n",
        "  print(f\"Classification report: \\n{classification_report(ytrain, ypred_train)}\")\n",
        "\n",
        "  print(\"\\nMeasures for test set\")\n",
        "  print(f\"Classification report: \\n{classification_report(ytest, ypred_test)}\")\n",
        "\n",
        "base_classification(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LHJk_OouD1R"
      },
      "source": [
        "* Duża różnica pomiędzy treningowym accuracy (`74%`), a testowym (`52%`)\n",
        "* `Precision` i `recall` są niskie zarówno dla treningowego zbioru, jak i testowego\n",
        "\n",
        "Tutaj możemy zaobserwować **overfitting**. Model radzi sobie zdecydowanie słabiej na testowym zbiorze w porównaniu do zbioru treningowego."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxyAZC_1u0-s"
      },
      "source": [
        "## Bardziej zaawansowana klasyfikacja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UeZ-ExZeJd1m",
        "outputId": "d14c5c9b-1f25-4826-ee50-176df51514b1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-02 18:45:53,639] A new study created in memory with name: no-name-6143d3cf-3230-42b1-b83e-dfeac83cfa5e\n",
            "[I 2025-06-02 18:46:07,778] Trial 0 finished with value: 0.6003154739996845 and parameters: {'k': 23, 'C': 38.57435496902186, 'gamma': 0.0031067193799950866}. Best is trial 0 with value: 0.6003154739996845.\n",
            "[I 2025-06-02 18:46:12,132] Trial 1 finished with value: 0.5531649636912794 and parameters: {'k': 21, 'C': 0.22518863034218248, 'gamma': 0.0008842550759683997}. Best is trial 0 with value: 0.6003154739996845.\n",
            "[I 2025-06-02 18:46:16,914] Trial 2 finished with value: 0.7504933662828401 and parameters: {'k': 12, 'C': 70.82023580407028, 'gamma': 0.026032932052262286}. Best is trial 2 with value: 0.7504933662828401.\n",
            "[I 2025-06-02 18:46:21,853] Trial 3 finished with value: 0.5524901998586209 and parameters: {'k': 23, 'C': 1.9465103810345263, 'gamma': 0.000309701197214266}. Best is trial 2 with value: 0.7504933662828401.\n",
            "[I 2025-06-02 18:46:27,077] Trial 4 finished with value: 0.6209994683678894 and parameters: {'k': 37, 'C': 52.8680342411861, 'gamma': 0.033841414496558316}. Best is trial 2 with value: 0.7504933662828401.\n",
            "[I 2025-06-02 18:46:34,063] Trial 5 finished with value: 0.6161686266949424 and parameters: {'k': 36, 'C': 12.93281047260291, 'gamma': 0.02183533719848606}. Best is trial 2 with value: 0.7504933662828401.\n",
            "[I 2025-06-02 18:46:38,087] Trial 6 finished with value: 0.7111572637888427 and parameters: {'k': 16, 'C': 16.791023309478508, 'gamma': 0.03704259034973933}. Best is trial 2 with value: 0.7504933662828401.\n",
            "[I 2025-06-02 18:46:41,565] Trial 7 finished with value: 0.6671521460995146 and parameters: {'k': 17, 'C': 14.039234412764097, 'gamma': 0.015389219275464982}. Best is trial 2 with value: 0.7504933662828401.\n",
            "[I 2025-06-02 18:46:45,076] Trial 8 finished with value: 0.7491543544175123 and parameters: {'k': 13, 'C': 5.73180566944422, 'gamma': 0.06850859113991015}. Best is trial 2 with value: 0.7504933662828401.\n",
            "[I 2025-06-02 18:46:50,144] Trial 9 finished with value: 0.5679881521986786 and parameters: {'k': 28, 'C': 81.59992899825662, 'gamma': 0.0007175057545368173}. Best is trial 2 with value: 0.7504933662828401.\n",
            "[I 2025-06-02 18:46:53,389] Trial 10 finished with value: 0.6124828387986283 and parameters: {'k': 10, 'C': 1.0124784667941733, 'gamma': 0.007217808427315202}. Best is trial 2 with value: 0.7504933662828401.\n",
            "[I 2025-06-02 18:46:56,139] Trial 11 finished with value: 0.7774891774891773 and parameters: {'k': 10, 'C': 5.142212107115787, 'gamma': 0.09806699810729616}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:00,095] Trial 12 finished with value: 0.7209883683567894 and parameters: {'k': 10, 'C': 0.3984866667313818, 'gamma': 0.08002254963781855}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:04,196] Trial 13 finished with value: 0.5863183600025706 and parameters: {'k': 31, 'C': 3.8625488700441353, 'gamma': 0.005163665064227305}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:07,322] Trial 14 finished with value: 0.706821248926512 and parameters: {'k': 17, 'C': 1.4446943111040724, 'gamma': 0.08966826304391157}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:11,262] Trial 15 finished with value: 0.6846592004486742 and parameters: {'k': 13, 'C': 29.830384303936086, 'gamma': 0.01111933021424665}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:15,509] Trial 16 finished with value: 0.5766479134900188 and parameters: {'k': 18, 'C': 7.051849217563158, 'gamma': 0.0026822647001900923}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:21,784] Trial 17 finished with value: 0.7503210240052345 and parameters: {'k': 13, 'C': 85.20531833171869, 'gamma': 0.037222958512678504}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:26,697] Trial 18 finished with value: 0.5793329477540003 and parameters: {'k': 28, 'C': 0.10167789973036792, 'gamma': 0.02354671628149471}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:30,396] Trial 19 finished with value: 0.5110012794223321 and parameters: {'k': 20, 'C': 0.6050751214081102, 'gamma': 0.00014304024827003069}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:34,482] Trial 20 finished with value: 0.5568197883987358 and parameters: {'k': 40, 'C': 2.7556713426392863, 'gamma': 0.0015641652176358045}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:43,221] Trial 21 finished with value: 0.7521612890033942 and parameters: {'k': 13, 'C': 93.69001935485824, 'gamma': 0.04433922767335052}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:47,866] Trial 22 finished with value: 0.7369975054185579 and parameters: {'k': 14, 'C': 26.401542074709777, 'gamma': 0.05132610133067643}. Best is trial 11 with value: 0.7774891774891773.\n",
            "[I 2025-06-02 18:47:54,847] Trial 23 finished with value: 0.8043313411734465 and parameters: {'k': 10, 'C': 94.68071978635496, 'gamma': 0.09430628073309341}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:47:57,626] Trial 24 finished with value: 0.7789911842543421 and parameters: {'k': 10, 'C': 7.106444027374362, 'gamma': 0.09322381198584474}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:00,432] Trial 25 finished with value: 0.7844927002821739 and parameters: {'k': 10, 'C': 7.9806090197131105, 'gamma': 0.09824040879751736}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:03,944] Trial 26 finished with value: 0.6554778554778554 and parameters: {'k': 15, 'C': 7.573182929069178, 'gamma': 0.013405761033589496}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:09,641] Trial 27 finished with value: 0.6846545267597899 and parameters: {'k': 19, 'C': 10.180640494718313, 'gamma': 0.052921770866590126}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:12,938] Trial 28 finished with value: 0.6194840247471826 and parameters: {'k': 11, 'C': 2.957332694252025, 'gamma': 0.0053460923912275575}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:17,664] Trial 29 finished with value: 0.6646651594020014 and parameters: {'k': 23, 'C': 25.266942673911288, 'gamma': 0.09748313921810871}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:25,108] Trial 30 finished with value: 0.7228286333549491 and parameters: {'k': 15, 'C': 42.37124636642164, 'gamma': 0.05681773843579613}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:27,787] Trial 31 finished with value: 0.7774891774891776 and parameters: {'k': 10, 'C': 4.733373639683214, 'gamma': 0.09566249130190242}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:30,927] Trial 32 finished with value: 0.7751529172581804 and parameters: {'k': 10, 'C': 18.644733014332186, 'gamma': 0.05882583030795339}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:35,468] Trial 33 finished with value: 0.7153256100624522 and parameters: {'k': 12, 'C': 9.407121612144355, 'gamma': 0.025702509328964}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:38,534] Trial 34 finished with value: 0.7541528646791805 and parameters: {'k': 12, 'C': 4.550180051176052, 'gamma': 0.06715672340091003}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:41,940] Trial 35 finished with value: 0.64531550321024 and parameters: {'k': 15, 'C': 1.8405636133532408, 'gamma': 0.018695583354067777}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:45,124] Trial 36 finished with value: 0.6368315894631683 and parameters: {'k': 21, 'C': 0.9737706610106633, 'gamma': 0.031930854808070966}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:50,619] Trial 37 finished with value: 0.7681511470985155 and parameters: {'k': 11, 'C': 54.95470616920603, 'gamma': 0.038687472970849825}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:53,838] Trial 38 finished with value: 0.5526450158029105 and parameters: {'k': 16, 'C': 2.437849091144415, 'gamma': 0.0005283884975195297}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:48:58,561] Trial 39 finished with value: 0.6101646891120575 and parameters: {'k': 33, 'C': 18.549930526058684, 'gamma': 0.009594702777191405}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:04,907] Trial 40 finished with value: 0.6416700258805521 and parameters: {'k': 26, 'C': 12.311563181100457, 'gamma': 0.07494393957882746}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:07,633] Trial 41 finished with value: 0.7768231768231769 and parameters: {'k': 10, 'C': 5.557859697636619, 'gamma': 0.09220563486825117}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:10,689] Trial 42 finished with value: 0.7644875592244014 and parameters: {'k': 12, 'C': 3.9229791047682414, 'gamma': 0.0953699206673344}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:14,730] Trial 43 finished with value: 0.7318219791904004 and parameters: {'k': 10, 'C': 6.859310448818137, 'gamma': 0.031777228117213854}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:17,995] Trial 44 finished with value: 0.7174836859047385 and parameters: {'k': 14, 'C': 1.637360062567749, 'gamma': 0.06410746205789851}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:20,905] Trial 45 finished with value: 0.7378300062510588 and parameters: {'k': 11, 'C': 3.938779636589077, 'gamma': 0.04624057671439613}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:24,245] Trial 46 finished with value: 0.7689842321421269 and parameters: {'k': 12, 'C': 9.296243643934366, 'gamma': 0.09846569006264189}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:28,524] Trial 47 finished with value: 0.6208160260791838 and parameters: {'k': 17, 'C': 1.0837002345030344, 'gamma': 0.016630915437444615}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:31,875] Trial 48 finished with value: 0.7336651652441125 and parameters: {'k': 14, 'C': 4.7591476832004895, 'gamma': 0.06991657854031}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:34,957] Trial 49 finished with value: 0.7349907402538982 and parameters: {'k': 10, 'C': 13.54928514450398, 'gamma': 0.026604404925469328}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:38,508] Trial 50 finished with value: 0.555988455988456 and parameters: {'k': 12, 'C': 0.23732582030005986, 'gamma': 0.0023251845178190807}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:42,572] Trial 51 finished with value: 0.7781545939440675 and parameters: {'k': 10, 'C': 5.701586557446139, 'gamma': 0.09658841870449725}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:46,075] Trial 52 finished with value: 0.7521601205811732 and parameters: {'k': 11, 'C': 2.2242103970869564, 'gamma': 0.07455134101094524}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:49,470] Trial 53 finished with value: 0.7486566065513435 and parameters: {'k': 10, 'C': 5.976114245901719, 'gamma': 0.045070000778977624}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:52,514] Trial 54 finished with value: 0.7523219470587891 and parameters: {'k': 12, 'C': 3.453402996213594, 'gamma': 0.06957101120401045}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:49:57,202] Trial 55 finished with value: 0.7345000029210556 and parameters: {'k': 14, 'C': 8.407094141805324, 'gamma': 0.05295555994095915}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:00,160] Trial 56 finished with value: 0.7283278125383389 and parameters: {'k': 11, 'C': 5.046677291962657, 'gamma': 0.03726158223348314}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:07,500] Trial 57 finished with value: 0.744998276577224 and parameters: {'k': 13, 'C': 64.61852543838208, 'gamma': 0.09920828173680354}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:11,335] Trial 58 finished with value: 0.55148009884852 and parameters: {'k': 16, 'C': 16.389857453849196, 'gamma': 0.00016058003597508894}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:14,446] Trial 59 finished with value: 0.7183237814816763 and parameters: {'k': 13, 'C': 1.2652428415963946, 'gamma': 0.05820759054874339}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:20,685] Trial 60 finished with value: 0.6891500312552944 and parameters: {'k': 18, 'C': 33.29053255051701, 'gamma': 0.07931324040358205}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:24,188] Trial 61 finished with value: 0.7736573368152315 and parameters: {'k': 10, 'C': 5.8914579192521925, 'gamma': 0.08313938168660455}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:26,869] Trial 62 finished with value: 0.772322998638788 and parameters: {'k': 10, 'C': 3.173241699226499, 'gamma': 0.09777756735786342}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:29,988] Trial 63 finished with value: 0.7546587915008967 and parameters: {'k': 11, 'C': 10.623343679839913, 'gamma': 0.04472401274761246}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:33,639] Trial 64 finished with value: 0.7529879477247897 and parameters: {'k': 13, 'C': 6.58539590312991, 'gamma': 0.07757956271683786}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:38,005] Trial 65 finished with value: 0.7738215001372896 and parameters: {'k': 11, 'C': 21.622082096882295, 'gamma': 0.061908619664706406}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:41,107] Trial 66 finished with value: 0.6809815915079073 and parameters: {'k': 10, 'C': 0.685943997255395, 'gamma': 0.030605532815835643}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:44,698] Trial 67 finished with value: 0.5563232089547879 and parameters: {'k': 25, 'C': 4.983300521374386, 'gamma': 0.0012512816694339275}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:49,099] Trial 68 finished with value: 0.6923217133743449 and parameters: {'k': 15, 'C': 2.677919949905074, 'gamma': 0.04016182917814225}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:54,526] Trial 69 finished with value: 0.6403269245374509 and parameters: {'k': 31, 'C': 7.711036786999877, 'gamma': 0.055778853160788365}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:50:58,421] Trial 70 finished with value: 0.7404986826039459 and parameters: {'k': 14, 'C': 11.77183625674628, 'gamma': 0.0800821682686379}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:51:02,214] Trial 71 finished with value: 0.7533250375355639 and parameters: {'k': 10, 'C': 4.268110985388075, 'gamma': 0.056999497712928414}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:51:06,790] Trial 72 finished with value: 0.787327876801561 and parameters: {'k': 11, 'C': 39.6525657612333, 'gamma': 0.09905916373803914}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:51:12,114] Trial 73 finished with value: 0.7756635177687808 and parameters: {'k': 12, 'C': 45.41250959404332, 'gamma': 0.09976908425730675}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:51:19,082] Trial 74 finished with value: 0.7893311366995578 and parameters: {'k': 11, 'C': 80.83669252761874, 'gamma': 0.08513178015029382}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:51:24,418] Trial 75 finished with value: 0.7828306196727249 and parameters: {'k': 11, 'C': 73.46278839336716, 'gamma': 0.0789278063264618}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:51:31,193] Trial 76 finished with value: 0.7646563962353438 and parameters: {'k': 12, 'C': 72.10663887181468, 'gamma': 0.04723075707216424}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:51:38,764] Trial 77 finished with value: 0.7481635323740585 and parameters: {'k': 13, 'C': 91.79749690950028, 'gamma': 0.0673101138101215}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:51:43,579] Trial 78 finished with value: 0.6543193648456807 and parameters: {'k': 11, 'C': 59.7271884424877, 'gamma': 0.0038253210070848514}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:51:50,289] Trial 79 finished with value: 0.7143342038078879 and parameters: {'k': 15, 'C': 47.40536869506693, 'gamma': 0.08071891021678797}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:51:56,854] Trial 80 finished with value: 0.7816586921850079 and parameters: {'k': 11, 'C': 77.31317118621908, 'gamma': 0.06494456116451507}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:52:02,107] Trial 81 finished with value: 0.7818263607737292 and parameters: {'k': 11, 'C': 79.72513655833784, 'gamma': 0.061725052397554}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:52:07,546] Trial 82 finished with value: 0.7814916078073972 and parameters: {'k': 11, 'C': 74.56297360697229, 'gamma': 0.06568015612081435}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:52:12,791] Trial 83 finished with value: 0.7524855261697366 and parameters: {'k': 11, 'C': 79.58936932769953, 'gamma': 0.020775184618095438}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:52:17,695] Trial 84 finished with value: 0.7519942046257834 and parameters: {'k': 13, 'C': 35.98030789862666, 'gamma': 0.0646312668720204}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:52:24,599] Trial 85 finished with value: 0.6196698038803301 and parameters: {'k': 39, 'C': 70.39869006035411, 'gamma': 0.028912298453079473}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:52:29,740] Trial 86 finished with value: 0.7718240823503981 and parameters: {'k': 11, 'C': 99.36785664465279, 'gamma': 0.038570786868691614}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:52:35,769] Trial 87 finished with value: 0.7693265798528958 and parameters: {'k': 12, 'C': 52.63311324333052, 'gamma': 0.07639868904730629}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:52:43,532] Trial 88 finished with value: 0.733159822633507 and parameters: {'k': 14, 'C': 74.0759744244948, 'gamma': 0.05020996969493461}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:52:48,595] Trial 89 finished with value: 0.7668226510331774 and parameters: {'k': 12, 'C': 39.05467592580536, 'gamma': 0.06283704379389232}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:52:55,806] Trial 90 finished with value: 0.6599961442066706 and parameters: {'k': 22, 'C': 27.362012200392407, 'gamma': 0.03455759950135626}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:53:00,723] Trial 91 finished with value: 0.7851622062148378 and parameters: {'k': 11, 'C': 57.913223111329735, 'gamma': 0.08673147997421979}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:53:06,398] Trial 92 finished with value: 0.7798277745646167 and parameters: {'k': 11, 'C': 55.82100886178406, 'gamma': 0.05304480999319662}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:53:10,720] Trial 93 finished with value: 0.7763230921125658 and parameters: {'k': 11, 'C': 50.456233084887906, 'gamma': 0.05125134550271664}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:53:18,570] Trial 94 finished with value: 0.7491625333730597 and parameters: {'k': 13, 'C': 61.95446696388912, 'gamma': 0.07971426812709503}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:53:25,094] Trial 95 finished with value: 0.7691606638975059 and parameters: {'k': 12, 'C': 87.07041264518965, 'gamma': 0.06482004148189029}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:53:34,253] Trial 96 finished with value: 0.7366621682411155 and parameters: {'k': 14, 'C': 99.77351595709206, 'gamma': 0.0412672979922311}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:53:39,097] Trial 97 finished with value: 0.7836642889274469 and parameters: {'k': 11, 'C': 57.32471403944692, 'gamma': 0.08391060031565008}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:53:46,044] Trial 98 finished with value: 0.74949553370606 and parameters: {'k': 13, 'C': 32.2755310593202, 'gamma': 0.08289907889409374}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:53:50,167] Trial 99 finished with value: 0.7924928872297294 and parameters: {'k': 10, 'C': 40.96452947753256, 'gamma': 0.07064246598617094}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:53:57,073] Trial 100 finished with value: 0.7015008383429436 and parameters: {'k': 16, 'C': 44.07433064996213, 'gamma': 0.0883780766719445}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:01,979] Trial 101 finished with value: 0.794157888894731 and parameters: {'k': 10, 'C': 79.62030131391975, 'gamma': 0.06577317245415927}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:06,166] Trial 102 finished with value: 0.7948244737718423 and parameters: {'k': 10, 'C': 63.42746509307638, 'gamma': 0.07197008070523324}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:10,549] Trial 103 finished with value: 0.793660141028562 and parameters: {'k': 10, 'C': 38.96511157291203, 'gamma': 0.07330976535984024}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:14,866] Trial 104 finished with value: 0.7978226451910663 and parameters: {'k': 10, 'C': 41.28323847236913, 'gamma': 0.08636698887761365}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:18,048] Trial 105 finished with value: 0.7914921335973967 and parameters: {'k': 10, 'C': 22.518199745569863, 'gamma': 0.08500243577207242}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:21,347] Trial 106 finished with value: 0.5843098422045789 and parameters: {'k': 10, 'C': 37.02084378533696, 'gamma': 0.0004720912422984853}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:25,877] Trial 107 finished with value: 0.7843250316934526 and parameters: {'k': 10, 'C': 21.774191953807023, 'gamma': 0.07291278746428584}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:29,260] Trial 108 finished with value: 0.7964883070146227 and parameters: {'k': 10, 'C': 29.104991599387454, 'gamma': 0.08946860682958552}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:32,522] Trial 109 finished with value: 0.681984681984682 and parameters: {'k': 10, 'C': 29.740179695598545, 'gamma': 0.007951904930141364}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:36,481] Trial 110 finished with value: 0.7616506300716828 and parameters: {'k': 12, 'C': 23.26375578270002, 'gamma': 0.047264340048070105}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:41,283] Trial 111 finished with value: 0.8033247454300084 and parameters: {'k': 10, 'C': 41.43849287866728, 'gamma': 0.09959374379540921}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:45,130] Trial 112 finished with value: 0.799324651956231 and parameters: {'k': 10, 'C': 43.92505031950222, 'gamma': 0.087384713634629}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:48,907] Trial 113 finished with value: 0.7919928025191183 and parameters: {'k': 10, 'C': 44.563833929905826, 'gamma': 0.07145969267393197}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:53,675] Trial 114 finished with value: 0.7764843343790712 and parameters: {'k': 10, 'C': 32.27641729031747, 'gamma': 0.05459593296093968}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:54:57,456] Trial 115 finished with value: 0.7924923030186187 and parameters: {'k': 10, 'C': 43.66815117260142, 'gamma': 0.07024923702498241}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:01,197] Trial 116 finished with value: 0.7926588031851189 and parameters: {'k': 10, 'C': 44.49585507963879, 'gamma': 0.06975455336463211}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:06,383] Trial 117 finished with value: 0.7916592179750075 and parameters: {'k': 10, 'C': 44.64534410166653, 'gamma': 0.07112585601566641}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:10,144] Trial 118 finished with value: 0.7716494032283505 and parameters: {'k': 10, 'C': 49.35705770873179, 'gamma': 0.042537375335942734}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:15,687] Trial 119 finished with value: 0.7666567350777876 and parameters: {'k': 12, 'C': 64.98895205745752, 'gamma': 0.05732022816472046}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:20,398] Trial 120 finished with value: 0.7853240326924537 and parameters: {'k': 10, 'C': 27.538748403587146, 'gamma': 0.06840250372016934}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:24,079] Trial 121 finished with value: 0.7933259722733409 and parameters: {'k': 10, 'C': 38.52632866397775, 'gamma': 0.07257256535372218}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:27,756] Trial 122 finished with value: 0.7938266411950623 and parameters: {'k': 10, 'C': 40.218104581785475, 'gamma': 0.07199032382803339}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:33,480] Trial 123 finished with value: 0.7656513077565709 and parameters: {'k': 12, 'C': 37.757766851636404, 'gamma': 0.04981333068950242}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:36,977] Trial 124 finished with value: 0.7808221018747336 and parameters: {'k': 10, 'C': 33.829505629379845, 'gamma': 0.05904162949204571}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:42,007] Trial 125 finished with value: 0.7718275876170614 and parameters: {'k': 12, 'C': 40.90972587736757, 'gamma': 0.09044526491096502}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:46,302] Trial 126 finished with value: 0.7841591157380631 and parameters: {'k': 10, 'C': 18.55724451132489, 'gamma': 0.07665520907219808}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:50,733] Trial 127 finished with value: 0.6421636842689473 and parameters: {'k': 35, 'C': 50.65857060762458, 'gamma': 0.07340513447797542}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:55:54,941] Trial 128 finished with value: 0.7729930887825625 and parameters: {'k': 12, 'C': 25.795449659901237, 'gamma': 0.09851225259327166}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:00,056] Trial 129 finished with value: 0.7638192217139587 and parameters: {'k': 11, 'C': 40.77136058839581, 'gamma': 0.03529140935019391}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:04,113] Trial 130 finished with value: 0.7889922942554523 and parameters: {'k': 10, 'C': 60.64004582076845, 'gamma': 0.05790801082374943}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:07,606] Trial 131 finished with value: 0.7871596240017291 and parameters: {'k': 10, 'C': 31.034539865605105, 'gamma': 0.06954443680654476}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:12,113] Trial 132 finished with value: 0.7764849185901816 and parameters: {'k': 10, 'C': 47.720691513807324, 'gamma': 0.04579499410637954}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:17,629] Trial 133 finished with value: 0.7833283675388939 and parameters: {'k': 11, 'C': 65.86871021172556, 'gamma': 0.07136961910776293}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:21,451] Trial 134 finished with value: 0.7994905679116207 and parameters: {'k': 10, 'C': 43.54548156955344, 'gamma': 0.08845300412598056}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:27,700] Trial 135 finished with value: 0.65166879903722 and parameters: {'k': 28, 'C': 29.419310285456817, 'gamma': 0.08752699276227906}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:31,657] Trial 136 finished with value: 0.7809938599412283 and parameters: {'k': 11, 'C': 34.654748981736226, 'gamma': 0.06054703992243474}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:35,611] Trial 137 finished with value: 0.7084868348026243 and parameters: {'k': 12, 'C': 51.367053146654214, 'gamma': 0.013368319141530194}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:40,121] Trial 138 finished with value: 0.5688136424978532 and parameters: {'k': 11, 'C': 38.542269571878734, 'gamma': 0.00010835854082302876}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:44,292] Trial 139 finished with value: 0.7549929602561182 and parameters: {'k': 13, 'C': 15.669052693692146, 'gamma': 0.08831189525963647}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:48,291] Trial 140 finished with value: 0.7813221865853445 and parameters: {'k': 10, 'C': 56.95797347725766, 'gamma': 0.05108662756913039}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:53,036] Trial 141 finished with value: 0.7943243890612312 and parameters: {'k': 10, 'C': 44.60387274126073, 'gamma': 0.07410369217024036}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:56:57,882] Trial 142 finished with value: 0.7883286304338937 and parameters: {'k': 11, 'C': 42.836263623658645, 'gamma': 0.09774734927350695}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:57:01,496] Trial 143 finished with value: 0.7941567204725098 and parameters: {'k': 10, 'C': 35.362453998690995, 'gamma': 0.07906038943600263}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:57:05,754] Trial 144 finished with value: 0.784160284160284 and parameters: {'k': 11, 'C': 25.459748432462174, 'gamma': 0.08049266729507512}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:57:11,753] Trial 145 finished with value: 0.6423366107576634 and parameters: {'k': 27, 'C': 66.73715946363866, 'gamma': 0.060006029566560655}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:57:15,807] Trial 146 finished with value: 0.7848274532485058 and parameters: {'k': 11, 'C': 34.40125862994232, 'gamma': 0.08699255095503976}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:57:19,731] Trial 147 finished with value: 0.6894818631660737 and parameters: {'k': 10, 'C': 0.10419849567467014, 'gamma': 0.07654522533132427}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:57:23,693] Trial 148 finished with value: 0.6516512727039042 and parameters: {'k': 10, 'C': 88.67153857819744, 'gamma': 0.002101444199047573}. Best is trial 23 with value: 0.8043313411734465.\n",
            "[I 2025-06-02 18:57:29,380] Trial 149 finished with value: 0.7756629335576704 and parameters: {'k': 12, 'C': 53.40201235228261, 'gamma': 0.09959161301005526}. Best is trial 23 with value: 0.8043313411734465.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best accuracy: 0.8043313411734465\n",
            "Best hyperparameters: {'k': 10, 'C': 94.68071978635496, 'gamma': 0.09430628073309341}\n"
          ]
        }
      ],
      "source": [
        "def random_forest_classification(df):\n",
        "  X = df.drop(['Class', 'Output'], axis=1).values\n",
        "  Y = df['Class'].values\n",
        "\n",
        "  rf = RandomForestClassifier(random_state=42)\n",
        "  rf.fit(X, Y)\n",
        "  importances = rf.feature_importances_\n",
        "\n",
        "  plt.bar(range(len(importances)), importances)\n",
        "  plt.title(\"Feature Importances\")\n",
        "  plt.show()\n",
        "\n",
        "  def objective(trial):\n",
        "    k = trial.suggest_int('k', 10, 40)\n",
        "    C = trial.suggest_float('C', 0.1, 100.0, log=True)\n",
        "    gamma = trial.suggest_float('gamma', 1e-4, 1e-1, log=True)\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('select', SelectKBest(score_func=f_classif, k=k)),\n",
        "        ('svc', SVC(kernel='rbf', C=C, gamma=gamma))\n",
        "    ])\n",
        "\n",
        "    rkf = RepeatedStratifiedKFold(n_splits=7, n_repeats=3, random_state=42)\n",
        "    scores = cross_val_score(pipeline, X, Y, scoring='accuracy', cv=rkf, n_jobs=-1)\n",
        "    return np.mean(scores)\n",
        "\n",
        "  study = optuna.create_study(direction='maximize')\n",
        "  study.optimize(objective, n_trials=150)\n",
        "\n",
        "  print(\"Best accuracy:\", study.best_value)\n",
        "  print(\"Best hyperparameters:\", study.best_params)\n",
        "\n",
        "  return study\n",
        "\n",
        "study = random_forest_classification(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXq1OgZHJt1W"
      },
      "source": [
        "Najpierw sprawdźmy jak dużo jest zmiennych, których istotność jest stosunkowo wysoka."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UcNl4q-J7zm"
      },
      "source": [
        "Widać, że zmiennych, których istotność jest stosunkowo wysoka jest niewiele. W dalszej części będziemy zatem budować model używając `SelectKBest`, który będzie nam wybierał `k` najistotniejszych zmiennych."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lERtLEVYKeV-"
      },
      "source": [
        "Sprawdzono różne przedziały wartości `k`, `C`, `gamma` i ostatecznie znaleziono najlepsze `k = 10`, `C = 66.06085092649397` oraz `gamma = 0.08569143608831382`, dla których osiągnięto `accuracy = 0.8024939972308394`.\n",
        "\n",
        "Testowano też modele zbudowane za pomocą klasifikatorów `RandomForest` i `MLPClassifier` jednak w pierwszym z nich połączenie `optuny`, walidacji krzyżowej i samego modelu było zbyt wolne i jedna próba w `optunie` trwała zbyt długo.\n",
        "\n",
        "DLa klasyfikatora `MLP` uzyskano niskie wartości `accuracy`: ok. `0.55`.\n",
        "\n",
        "Ostatecznie klasyfikator `SVC` z jądrem `rbf` okazał się dawać najlepsze wyniki,a pojedyncze próby w `optunie` liczą się szybko. Oczywiście trzeba było wybrać najbardziej istotne zmienne i do tego celu użyto funkcji `SelectKBest`.\n",
        "\n",
        "Do walidacji krzyżowej użyto `RepeatedStratifiedKFold`, aby zapewnić, że procenty poszczególnych klas w każdej części podziału będą podobne i żadna klasa nie będzie przeważać nad drugą."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOOkiysnl0aN",
        "outputId": "fa4b794e-2666-4b08-d82e-3322cd729953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features in each fold:\n",
            "[array([ 39,  40,  61,  73, 237, 239, 292, 307, 329, 395]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([ 39,  40,  73, 237, 239, 245, 292, 307, 329, 395]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([  1,  39,  40, 237, 239, 266, 292, 307, 329, 395]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([ 39,  40,  73, 217, 239, 292, 307, 329, 395, 396]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([ 39,  40, 237, 239, 292, 307, 329, 333, 395, 396]), array([  1,  39,  40, 237, 239, 245, 292, 307, 329, 395]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([  1,  39,  40,  73, 237, 239, 292, 307, 329, 395]), array([ 39,  40,  73, 237, 239, 292, 307, 329, 395, 396]), array([  1,  39,  40, 237, 239, 266, 292, 307, 329, 395]), array([ 39,  40,  61,  73, 237, 239, 292, 307, 329, 395])]\n",
            "Mean accuracy: 0.7797860736265702\n",
            "Train accuracy: 0.872\n",
            "Difference: 0.092\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.85      0.88       202\n",
            "           1       0.86      0.92      0.89       198\n",
            "\n",
            "    accuracy                           0.89       400\n",
            "   macro avg       0.89      0.89      0.88       400\n",
            "weighted avg       0.89      0.89      0.88       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def random_forest_classification_best_model(df, study):\n",
        "  X = df.drop(['Class', 'Output'], axis=1).values\n",
        "  Y = df['Class'].values\n",
        "\n",
        "  xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "  params = study.best_params\n",
        "\n",
        "  pipeline = Pipeline([\n",
        "      ('scaler', StandardScaler()),\n",
        "      ('select', SelectKBest(score_func=f_classif, k=params['k'])),\n",
        "      ('svc', SVC(kernel='rbf', C=params['C'], gamma=params['gamma']))\n",
        "  ])\n",
        "\n",
        "  rkf = RepeatedStratifiedKFold(n_splits=7, n_repeats=3, random_state=42)\n",
        "\n",
        "  from sklearn.model_selection import cross_validate\n",
        "\n",
        "  cv_results = cross_validate(\n",
        "      pipeline,\n",
        "      xtrain, ytrain,\n",
        "      cv=rkf,\n",
        "      scoring='accuracy',\n",
        "      n_jobs=-1,\n",
        "      return_estimator=True\n",
        "  )\n",
        "\n",
        "  selected_features = []\n",
        "  for estimator in cv_results['estimator']:\n",
        "      selector = estimator.named_steps['select']\n",
        "      selected_features.append(selector.get_support(indices=True))\n",
        "\n",
        "  print(\"Selected features in each fold:\")\n",
        "  print(selected_features)\n",
        "  test_score = np.mean(cv_results['test_score'])\n",
        "  print(f\"Mean accuracy: {test_score}\")\n",
        "\n",
        "  pipeline.fit(X, Y)\n",
        "  train_score = accuracy_score(Y, pipeline.predict(X))\n",
        "  print(f\"Train accuracy: {train_score:.3f}\")\n",
        "  print(f\"Difference: {(train_score - test_score):.3f}\")\n",
        "\n",
        "  print(f\"Classification report:\\n{classification_report(ytest, pipeline.predict(xtest))}\")\n",
        "\n",
        "random_forest_classification_best_model(df, study)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoaHtu1YP_uH"
      },
      "source": [
        "Tutaj został zbudowany model na podstawie najlepszych parametrów znalezionych wcześniej za pomocą `optuny`. Zastosowano zamiast `cross_val_score` funkcję `cross_validate`, ponieważ w ten sposób łatwiej wyciągnąć z `pipeline` informację o zmiennych, które zostały użyte.\n",
        "\n",
        "Zatem zostały użyte zmienne o numerach:\n",
        "`1`, `39`, `40`, `73`, `237`, `239`, `292`, `307`, `329`, `395`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GmIkc32Sd0w"
      },
      "source": [
        "Sprawdzamy także `accuracy` dla zbioru treningowego. Widać, że różnica między `accuracy` dla zbioru treningowego, a `accuracy` dla zbioru testowego wynoszące `0.056` mówi nam, że i tak lekki `overfitting` występuje. Jednak patrząc na to, że `accuracy` dla zbioru testowego wynosi `0.805`, to otrzymaliśmy i tak w miarę zadowalający model.\n",
        "\n",
        "Dla naszego bazowego modelu ta różnica wynosiła ok. `0.22` i tam mogliśmy mówić o zdecydowanym overfittingu. Nasz bardziej zaawansowany model overfittuje zdecydowanie mniej.\n",
        "\n",
        "Widać także, że `precision`, `recall` i `f1-score` są dużo większe niż w modelu bazowym, więc to nam mówi, że ten model jest generalnie lepszy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dElKWaKj2cOo"
      },
      "source": [
        "## Bardziej zaawansowana regresja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwqCn9893E78"
      },
      "source": [
        "Stworzymy model korzystający z klasyfikatora `Lasso`. Wcześniej należy przeskalować dane. Używamy `GridSearchCV` w celu znalezienia najlepszych parametrów modelu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "id": "79J553W22lFU",
        "outputId": "b1d26c2d-76e0-49da-f1ea-0b5388b72525"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"67be3847-2dc2-414a-8e77-81e14d4c83c5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"67be3847-2dc2-414a-8e77-81e14d4c83c5\")) {                    Plotly.newPlot(                        \"67be3847-2dc2-414a-8e77-81e14d4c83c5\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.01,0.016964285714285716,0.02392857142857143,0.030892857142857146,0.03785714285714286,0.044821428571428575,0.05178571428571429,0.058750000000000004,0.06571428571428571,0.07267857142857143,0.07964285714285714,0.08660714285714285,0.09357142857142857,0.10053571428571428,0.1075,0.11446428571428571,0.12142857142857143,0.12839285714285714,0.13535714285714287,0.1423214285714286,0.1492857142857143,0.15625,0.16321428571428573,0.17017857142857146,0.17714285714285716,0.18410714285714286,0.1910714285714286,0.19803571428571431,0.20500000000000002,0.21196428571428572,0.21892857142857144,0.22589285714285717,0.23285714285714287,0.23982142857142857,0.2467857142857143,0.25375000000000003,0.26071428571428573,0.26767857142857143,0.2746428571428572,0.2816071428571429,0.2885714285714286,0.2955357142857143,0.3025,0.30946428571428575,0.31642857142857145,0.32339285714285715,0.3303571428571429,0.3373214285714286,0.3442857142857143,0.35125,0.3582142857142857,0.36517857142857146,0.37214285714285716,0.37910714285714286,0.3860714285714286,0.3930357142857143,0.4],\"xaxis\":\"x\",\"y\":[-2.823333802702216,-2.7652824264789,-2.7205676567196475,-2.6853082506455763,-2.658105540973302,-2.6386132254051007,-2.6239383164505883,-2.613142558301975,-2.606072746063506,-2.6009037732731466,-2.5952779743631607,-2.5907632496995827,-2.587610081371266,-2.5859985850851284,-2.585237881769143,-2.585290665086177,-2.5864054256957787,-2.5888980090157694,-2.592485107665399,-2.5967818801301754,-2.6016897347030117,-2.60759557193047,-2.6140870686259903,-2.6211785565120196,-2.628636570039563,-2.636484970806914,-2.644742989563991,-2.6534470778047923,-2.6624780198697833,-2.6717149217487783,-2.6810250981414128,-2.690295343266511,-2.699425633258385,-2.7085629224093664,-2.717724674459704,-2.7270427431643918,-2.7366126882022863,-2.7464555505244745,-2.7563111344541324,-2.765328836135952,-2.773963860510293,-2.782171768732048,-2.7901435267364496,-2.797895449874307,-2.805670745686666,-2.813553160513507,-2.821577852740583,-2.8297438488350024,-2.8380245324092326,-2.8463573343462945,-2.854700191845647,-2.863194039323669,-2.8718267726274522,-2.8805966357803854,-2.889510910232354,-2.898567076217311,-2.9076698597725907],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Param Alpha\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"RMSE\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Lasso negRMSE\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('67be3847-2dc2-414a-8e77-81e14d4c83c5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best alpha: 0.1075\n",
            "Non-zero coefficients: 43\n",
            "Used predictors:\n",
            "17, 24, 40, 43, 58, 66, 67, 82, 93, 97, 99, 122, 130, 131, 135, 155, 166, 171, 172, 183, 192, 193, 203, 212, 222, 230, 240, 249, 269, 280, 284, 285, 291, 297, 299, 303, 323, 334, 341, 361, 386, 387, 388, \n",
            "\n",
            "Training set results\n",
            "RMSE: 2.601 +/- 0.186\n",
            "\n",
            "Validation set results\n",
            "RMSE: 2.710 +/- 0.102\n",
            "\n",
            "Test set results\n",
            "RMSE: 2.433\n",
            "\n",
            "Difference: 0.169\n",
            "\n",
            "R^2\n",
            "Training set: 0.506\n",
            "Test set: 0.533\n"
          ]
        }
      ],
      "source": [
        "def lasso_regression(df):\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  X = df.drop(['Class', 'Output'], axis=1).values\n",
        "  Y = df['Output'].values\n",
        "\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "  xtrain, xtest, ytrain, ytest = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # pca = PCA(n_components=0.95)\n",
        "  # xtrain_pca = pca.fit_transform(xtrain)\n",
        "  # xtest_pca = pca.transform(xtest)\n",
        "\n",
        "  def run_grid_search(model, xtrain, ytrain, mini, maxi):\n",
        "      param_grid = {'alpha': np.linspace(mini, maxi, num=57)}\n",
        "\n",
        "      grid_search = GridSearchCV(\n",
        "          estimator = model,\n",
        "          param_grid = param_grid,\n",
        "          scoring = 'neg_mean_squared_error',\n",
        "          cv = 10,\n",
        "          return_train_score=True\n",
        "      )\n",
        "\n",
        "      grid_search.fit(xtrain, ytrain)\n",
        "\n",
        "      return grid_search\n",
        "\n",
        "  def print_results(grid_search, name, xtrain, xtest):\n",
        "      results = grid_search.cv_results_\n",
        "      results_df = pd.DataFrame(results)\n",
        "      results_df['RMSE'] = np.sqrt(-results_df['mean_test_score'])\n",
        "\n",
        "      fig = px.scatter(\n",
        "          x=results_df['param_alpha'],\n",
        "          y=-results_df['RMSE'],\n",
        "          title=f'{name} negRMSE'\n",
        "      )\n",
        "      fig.update_layout(xaxis_title='Param Alpha', yaxis_title='RMSE')\n",
        "      fig.show()\n",
        "\n",
        "      best_model = grid_search.best_estimator_\n",
        "      best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "      gs_results = pd.DataFrame(grid_search.cv_results_)\n",
        "      gs_results['Train_RMSE'] = np.sqrt(-gs_results['mean_train_score'])\n",
        "      gs_results['Val_RMSE'] = np.sqrt(-gs_results['mean_test_score'])\n",
        "\n",
        "      print(f\"Best alpha: {best_alpha}\")\n",
        "      print(\"Non-zero coefficients:\", np.sum(grid_search.best_estimator_.coef_ != 0))\n",
        "\n",
        "      print(\"Used predictors:\")\n",
        "      i = 0\n",
        "      for i in range(len(grid_search.best_estimator_.coef_)):\n",
        "        if grid_search.best_estimator_.coef_[i] != 0:\n",
        "          print(i, end=', ')\n",
        "      print()\n",
        "\n",
        "      print(f\"\\nTraining set results\")\n",
        "      print(f\"RMSE: {np.mean(gs_results['Train_RMSE']):.3f} +/- {np.std(gs_results['Train_RMSE']):.3f}\")\n",
        "\n",
        "      print(f\"\\nValidation set results\")\n",
        "      print(f\"RMSE: {np.mean(gs_results['Val_RMSE']):.3f} +/- {np.std(gs_results['Val_RMSE']):.3f}\")\n",
        "\n",
        "      print(f\"\\nTest set results\")\n",
        "      ypred_test = best_model.predict(xtest)\n",
        "      test_rmse = np.sqrt(mean_squared_error(ytest, ypred_test))\n",
        "      print(f\"RMSE: {test_rmse:.3f}\")\n",
        "\n",
        "      print(f\"\\nDifference: {abs(np.mean(gs_results['Train_RMSE']) - test_rmse):.3f}\")\n",
        "\n",
        "      print(f\"\\nR^2\")\n",
        "      print(f\"Training set: {r2_score(ytrain, best_model.predict(xtrain)):.3f}\")\n",
        "      print(f\"Test set: {r2_score(ytest, best_model.predict(xtest)):.3f}\")\n",
        "\n",
        "\n",
        "  lasso = Lasso()\n",
        "  grid_search = run_grid_search(lasso, xtrain, ytrain, 0.01, 0.4)\n",
        "  print_results(grid_search, \"Lasso\", xtrain, xtest)\n",
        "\n",
        "lasso_regression(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKAFNHUvMkF5"
      },
      "source": [
        "Spróbujemy stworzyć jeszcze model za pomocą `ElasticNet`, który łączy ze sobą klasyfikatory `Ridge` i `Lesso`. Również używamy `GridSearchCV` w celu znalezienia najlepszych parametrów modelu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi6LEYRJVbq3",
        "outputId": "d748321d-29ea-4f34-e801-72528bbbcc74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal ElasticNet parameters: {'alpha': np.float64(0.5130817610062893), 'l1_ratio': 0.1}\n",
            "Training RMSE: 2.489\n",
            "Validation RMSE: 2.767\n",
            "Test RMSE: 2.586\n",
            "Difference: 0.097\n",
            "Non-zero coefficients: 182\n",
            "Training R²: 0.493\n",
            "Test R²: 0.472\n",
            "Non-zero coefficients: 182\n"
          ]
        }
      ],
      "source": [
        "def elasticnet_regression(df):\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  X = df.drop(['Class', 'Output'], axis=1).values\n",
        "  Y = df['Output'].values\n",
        "\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "  xtrain, xtest, ytrain, ytest = train_test_split(\n",
        "      X_scaled,\n",
        "      Y,\n",
        "      test_size=0.2,\n",
        "      random_state=42\n",
        "  )\n",
        "\n",
        "  elastic_params = {'alpha': np.linspace(0.01, 80, 160), 'l1_ratio': [0.1, 0.5, 0.9]}\n",
        "  elastic = ElasticNet(max_iter=50000)\n",
        "  elastic_cv = GridSearchCV(elastic, elastic_params, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
        "  elastic_cv.fit(xtrain, ytrain)\n",
        "\n",
        "  elastic_results = pd.DataFrame(elastic_cv.cv_results_)\n",
        "  elastic_results['Train_RMSE'] = np.sqrt(-elastic_results['mean_train_score'])\n",
        "  elastic_results['Val_RMSE'] = np.sqrt(-elastic_results['mean_test_score'])\n",
        "\n",
        "  print(f\"Optimal ElasticNet parameters: {elastic_cv.best_params_}\")\n",
        "\n",
        "  best_idx = elastic_cv.best_index_\n",
        "  train_rmse = elastic_results.loc[best_idx, 'Train_RMSE']\n",
        "  val_rmse = elastic_results.loc[best_idx, 'Val_RMSE']\n",
        "  print(f\"Training RMSE: {train_rmse:.3f}\")\n",
        "  print(f\"Validation RMSE: {elastic_results.loc[best_idx, 'Val_RMSE']:.3f}\")\n",
        "  ypred = elastic_cv.predict(xtest)\n",
        "  test_rmse = np.sqrt(mean_squared_error(ytest, ypred))\n",
        "  print(f\"Test RMSE: {test_rmse:.3f}\")\n",
        "  print(f\"Difference: {abs(train_rmse - test_rmse):.3f}\")\n",
        "\n",
        "  best_elastic = elastic_cv.best_estimator_\n",
        "\n",
        "  coeffs = best_elastic.coef_\n",
        "  print(f\"Non-zero coefficients: {np.sum(coeffs != 0)}\")\n",
        "\n",
        "  r2_train = best_elastic.score(xtrain, ytrain)\n",
        "  r2_test = best_elastic.score(xtest, ytest)\n",
        "\n",
        "  print(f\"Training R²: {r2_train:.3f}\")\n",
        "  print(f\"Test R²: {r2_test:.3f}\")\n",
        "\n",
        "  coeffs = best_elastic.coef_\n",
        "  print(f\"Non-zero coefficients: {np.sum(coeffs != 0)}\")\n",
        "\n",
        "elasticnet_regression(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdnqs1RFZkr6"
      },
      "source": [
        "Dla przypomnienia w naszym bazowym modelu wyszły nam takie miary:\n",
        "\n",
        "| | Regresja liniowa | Lasso | ElasticNet |\n",
        "| --- | --- | --- | --- |\n",
        "| (RMSE) Zbiór treningowy | 2.209 | 2.601 | 2.489 |\n",
        "| (RMSE) Zbiór testowy | 2.647 | 2.433 | 2.586 |\n",
        "| (RMSE) Różnica (train vs test) | 0.438 | 0.169 | 0.097 |\n",
        "| R^2 (test) | 0.615 | 0.533 | 0.472 |\n",
        "\n",
        "Udało się uzyskać model za pomocą klasyfikatora `Lasso`, dla którego `R^2 > 0.5`. Przy tym `RMSE` jest trochę większe dla zbioru treningowego w porównaniu do modelu bazowego, ale za to `RMSE` dla zbioru testowego jest mniejsze. Zauważamy, że dla `Lasso` różnica pomiędzy `RMSE` treningowym i testowym jest `2,6` razy mniejsza niż w modelu bazowym.\n",
        "\n",
        "W modelu z `ElasticNet` różnica pomiędzy `RMSE` jest jeszcze mniejsza, ale za to tracimy na `R^2` (jest mniejsze niż `0.5`).\n",
        "\n",
        "W modelu z `Lasso` okazuje się, że najlepiej jest zostawić `43` predyktory (nie wyzerować współczynników dla nich). Jest to dużo mniej niż w modelu bazowym, w którym korzystamy z `400` predyktorów i modelu z `ElasicNet`, w ktorym korzystamy ze `182` predyktorów.\n",
        "\n",
        "Próbowano też zredukować wymiar danych przed użyciem `Lasso` za pomocą `PCA`, jednak pogorszyło to `R^2`.\n",
        "\n",
        "W `Lasso` użyto predyktorów o numerach:\n",
        "17, 24, 40, 43, 58, 66, 67, 82, 93, 97, 99, 122, 130, 131, 135, 155, 166, 171, 172, 183, 192, 193, 203, 212, 222, 230, 240, 249, 269, 280, 284, 285, 291, 297, 299, 303, 323, 334, 341, 361, 386, 387, 388"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iquODCh2E_Bv"
      },
      "outputs": [],
      "source": [
        "def validation_data(file_name='validation_data.csv'):\n",
        "  df = pd.read_csv(file_name, sep=';')\n",
        "\n",
        "  print(\"\\n=====BASELINE MODEL FOR REGRESSION=====\")\n",
        "  base_regression(df)\n",
        "\n",
        "  print(\"\\n=====BASELINE MODEL FOR CLASSIFICATION=====\")\n",
        "  base_classification(df)\n",
        "\n",
        "  print(\"\\n=====MORE ADVANCED MODEL FOR CLASSIFICATION=====\")\n",
        "  random_forest_classification(df)\n",
        "\n",
        "  print(\"\\n=====1. MORE ADVANCED MODEL FOR REGRESSION=====\")\n",
        "  lasso_regression(df)\n",
        "\n",
        "  print(\"\\n=====2. MORE ADVANCED MODEL FOR REGRESSION=====\")\n",
        "  elasticnet_regression(df)\n",
        "\n",
        "validation_data()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
